[
    {
        "content": "\n                    <p>Here we are, my very first blog post. The main objective of\n                        this post is to give a little taste of who I am as a person\n                        and also motivate the existence of this site.</p>\n\n                    <p>So who am I? Well, at risk of plagiarising my own Linked-In profile, I am a graduate student at the University of Virginia\n                        currently pursuing my Masters in Data Science. I chose to pursue this degree as I saw Data Science as the most cutting-edge manifestation of the subjects I studied as an undergrad\n                        (I double majored in Computer Science and Mathematics with a concentration in Probability and Statistics, also at the University of Virginia). I will complete\n                        this degree in May 2018, so I am also going through the trials\n                        and tribulations of a first-time job seeker. That being said, I do\n                        not want to get into too much more detail about myself here,\n                        because that is what this site is about! Exploring it should give\n                        you a relatively complete picture of who I am and what I am\n                        all about.</p>\n\n                    <p>So why exactly am I making a site for myself? I've listed\n                        a few compelling reasons on the homepage of this site,\n                        but really the motivation for this site came from enough\n                        people telling me that it would be a good idea to have\n                        a personal site. I put off making one for various reasons,\n                        but once I discovered that I already used (Github), had\n                        a mechanism for hosting static websites for free (Github Pages),\n                        I was officially out of excuses. I can think of a few additional reasons beyond what has already been listed. I think the exercise of publishing content could very well change the way\n                        I interact with the world. Whenever I am undergoing some\n                        interesting or challenging problem in my life, I can view it through the lens of something that can be written up and\n                        posted to the world. I imagine that this will\n                        help me engage with the world around me. Publishing is also\n                        a good mechanism to help hold myself accountable. My next\n                        blog post is going to involve my 2017-2018 New Years resolutions and the hope is that putting them out in a more\n                        permanent way online will help me to actually remember and\n                        follow through on them.</p>\n\n                    <p>So what exactly do I want this site to be? At the end of the day,\n                        I just it to be the most complete digital picture of myself out\n                        there, and one that I have complete and total control over.\n                        Beyond that, I do not know what this site could become. Most\n                        notably, I do not want to set any sort of goal or theme for this\n                        blog. It will certainly contain updates on my Capstone research\n                        and experience as a first-time job hunter, but I do not want to\n                        limit myself to those things. This is really an outlet to share\n                        anything in my life that I think others might find useful.</p>\n\n                    <p>Will that in mind, I hope that you will explore the rest of my site and I can't wait to see how this project evolves!</p>\n                    ",
        "date": "12/23/2017",
        "location": "blog/2017/introduction.html",
        "name": "Introduction Blog"
    },
    {
        "content": "\n                    <p>I'm going to be honest here, I am not the biggest fan of New\n                        Years Resolutions. Don't get me wrong, I think the idea of\n                        identifying areas for self-improvement and working\n                        towards those goals is great, I just think waiting for\n                        a calendar to tell you \"it's time to improve yourself\"\n                        is a bit silly. However, I do think the New Years at least\n                        provides a time to sit back and reflect, especially since\n                        it is around this time that people, myself included, have\n                        some time off and therefore can spend time reflecting.\n                        I've decided this year to publish my New Years Resolutions,\n                        mostly so I have a tangible and public set of goals, so\n                        anyone that reads this and knows me can hold me accountable to these resolutions. Also, I frankly do not remember what\n                        my resolutions last year were, so having them easily accessible\n                        should increase my chances of actually doing them.</p>\n\n                    <p>Without further ado, let's jump into my New Years Resolutions for 2018.<p>\n                    <ol>\n                        <li> <strong>Consume less, create more.</strong> I do not\n                            consider myself a procrastinator. In fact, I am pretty good\n                            at getting things done well before they need to be done.\n                            However, I am quite adept at wasting time. Too much of\n                            my day is spent consuming content that adds no real\n                            value to my life. Youtube and Netflix are the worst offenders here, with\n                            Facebook and Snapchat being close runner-ups. A simple\n                            way to achieve this goal would be to remove myself from\n                            those platforms, but I do not see that as realistic. There are\n                            very good things those platforms have to offer, I just need\n                            to be better at practicing moderation. More tangibly,\n                            I need to not use them just as a means to kill time. What\n                            do I plan on doing with all the time I'll be getting back?\n                            Well, create more, whatever that may look like. This\n                            website is a great start, so maintaining and growing it\n                            out will be a good way to work towards this goal. Getting back\n                            into writing music, and maybe this time actually record some\n                            of it, will also be valuable in achieving this goal.\n                            Continuing to work on the <i>projmanr</i> R package and\n                            <i>Super Smash Bros. SE</i> game mod will also be productive as well.</li>\n\n                        <li><strong>Invest more in people.</strong> I admit it,\n                            I can be a bit of a workaholic. For easily the past decade\n                            of my life, school has been the most important aspect\n                            of my life, especially my last four years in college.\n                            This emphasis on my education has really led me\n                            to neglect the people in my life, especially my group\n                            of friends.\n                            But I am really starting to see that while learning is\n                            incredibly important, school itself really isn't. Well,\n                            let me rephrase that. Certain very time-consuming\n                            aspects of school, like grades, really aren''t that important.\n                            My time would be much better spent ensuring that I have\n                            learned the material, and not worrying that much about\n                            grades at all. All the time I can save here can be reinvested\n                            in the people around me. And even if I am not\n                            successful in truly convincing myself to put less time into\n                            my schoolwork, there is still a whole lot of wasted time\n                            in my life (see 1), that I can get a hold of and redirect to\n                            the people that I care about (and even finding new people\n                            to care about!).\n                        </li>\n                        <li><strong>Change the way I eat food.</strong> No, I am not\n                            going on a diet. This resolution has nothing to do with what\n                            or how much I eat, but rather <i>why</i> I eat. I am very happy\n                            with my weight and overall think I have a pretty strong diet.\n                            But I do, far too often, find myself eating for reasons other than\n                            \"I'm hungry.\" The most sinister of these is eating out of habit.\n                            If I normally get a milkshake from Five Guys on Mondays, then\n                            I often find myself getting a milkshake from Five Guys on Mondays, even if I am not hungry in the least! These patterns\n                            are going to be hard to break, but ultimately it is very important\n                            that I do it now, when my metabolism is still forgiving. There\n                            are other reasons for eating, like stress, boredom, or socializing\n                            that I hope to break, but if I can eradicate habitual eating, I will\n                            consider this resolution a success. </li>\n                    </ol>\n\n                    <p>So there they are. I intentionally left my resolutions a little bit vague, without too many specific ways to achieve them. This is because my life at the moment can be so sporadic that I\n                        think getting to nitty-gritty with my resolutions will sharply\n                        decrease my chance of attaining them. I intend to post\n                        updates as the year progresses. Happy New Year!\n                        ",
        "date": "12/26/2017",
        "location": "blog/2017/resolutions.html",
        "name": "2017 New Years Resolutions"
    },
    {
        "content": "\n                    <p>One of the principal components (pun intended) of my Master's\n                        program at UVA is the completion of a Capstone research project.\n                        Now that I am halfway through the program, I feel it is a good time\n                        to do an overview of the project, explain what work we have\n                        completed so far, and a few things I've picked up along the way.</p>\n\n                    <h3>Overview</h3>\n\n                    <p>\n                        Our project is titled, \"Using Speech to Predict Violence in Value-Based Groups,\" or something like that. Basically, humans like to collate around common ideas, or \"values\", in our case. Being that\n                        humans are intrinsically social creatures, this formation of groups\n                        is great!... most of the time. Sometimes, members of these groups,\n                        inspired by ideas within the group, decide to commit acts of violence. Given that new groups like this are popping up all the time in less understood areas of the globe, it would be life-saving to figure out what groups are going\n                        to be violent before we (our military) has to interact with them. That\n                        is the crux of our research.\n                    </p>\n\n                    <p>\n                        Our capstone is actually the continuation of two years of prior\n                        capstone projects. These capstone project aimed to predict a metric\n                        assigned to groups called \"linguistic rigidity.\" Linguistic rigidity\n                        is given to texts from a given group and aims to quantify the, well,\n                        rigidity of the language a group uses. It's a bit unintuitive but the\n                        idea is that this \"linguistic rigidity\" is a proxy for the violent potential of a group. These groups showed promising results but\n                        we felt that their analyses had a couple of areas for improvement. The first being\n                        that linguistic rigidity seemed like an unnecessary middleman for\n                        the thing we actually care about, which is predicting group violence.\n                        Second, the analysis was restricted to English. The researchers used\n                        their knowledge of English to help inform the models what to look\n                        for. This is problematic since many of the groups appear in parts of\n                        the globe that do not primarily speak English, thus their analysis is\n                        largely void for the majority of the world.\n                    </p>\n\n                    <p>\n                        Our capstone hopes to address both of these issues. Instead of trying to predict linguistic rigidity, we are going straight to trying to predict\n                        violence. Further, we are conducting our experiment in a \"language\n                        agnostic\" manner. What this means is that nothing in our analysis is\n                        specific to the language being used. Theoretically, you should be able\n                        to plug in any language into our pipeline and get comparable results.\n                        As a proof of concept, we are conducting our analysis in Arabic,\n                        which none of us speak (well, one of our advisors does, but what are\n                        you gonna do?).\n                    </p>\n\n                    <h3>Definitions, Definitions, Definitions</h3>\n\n                    <p>Before we can get into the collections or our data, we need\n                        to establish some definitions. This has been a first for me since most\n                        of the projects I do are pretty straightforward and objective. But\n                        since this project falls more into the domain of human behavior,\n                        we need to establish some definitions. A fair question would be\n                        how we decided on definitions. The short answer is, we research\n                        how other people defined them and tweak the definitions to be\n                        most sensible for our use case. A fair followup would be, \"but\n                        doesn't that introduce some experimenter bias?\" Yes, but anyway you define subjective term will introduce bias, so let's accept that and move on.</p>\n\n                    <p>We know we need to find text's from violent and non-violent value\n                        based groups. But what the hell do most of the words in that last sentence mean in a practical sense? In the interest of not dragging on, I'm just going to list the answers to a bunch of preliminary questions we faced.</p>\n                    <ul style=\"padding: 0em 5em\">\n                        <li>\n                            <i>What is violence?</i> We ended up basing our definition\n                            of violence on a few different definitions we found. We define\n                            violence to be any intentional act that has a high probability of causing injury or death to humans or animals.\n                        </li>\n                        <li>\n                            <i>What is a violent group?</i> A violent group is any group who has\n                            members that commit an act of violence as described above and the group takes responsibility for the action.\n                        </li>\n                        <li>\n                            <i>What is a value?</i> Pretty much anything that somebody thinks.\n                        </li>\n                        <li>\n                            <i>What is a value based group?</i> Any group that operates under\n                            a common name and has either an explicit publically available set\n                            of motives/values or whose values are obvious from their behavior.\n                            The group must also have a primary motive outside of making a profit.\n                        </li>\n                        <li>\n                            <i>What is a document?</i> Any media that a group or members of a group put out that can be extracted into a text document. This includes web pages, PDFs, speeches (if we ever figure out a scalable way to do speech to text).</li>\n                        <li>\n                            <i>How are documents labeled?</i> All documents within a group are given the same label. So if ISIS is labeled as violent, all documents from ISIS are considered violent documents\n                        </li>\n                        <li>\n                            <i>Is that realistic?</i> Probably not, but it's a start. Future work, either by us or other researchers, should focus on creating a more\n                            granular definition of violence that can allow documents to be\n                            judged at a more individual level. This will introduce far less bias\n                            as groups change from non-violent to violent, or vice-versa, pretty frequently.\n                        </li>\n                        <li>\n                            <i>What does it mean for an analysis to be language agnostic?</i> The analysis should not do anything that is specific to a language\n                            and cannot be extended to another language. For example, part of speech tagging tools are not language agnostic as they do not\n                            exist readily for all languages. However, stop-word analysis is valid since this really just requires finding what words are used\n                            extremely often, which can be done with any language.\n                        </li>\n                        <li>\n                            <i>Can a person be a group?</i> Yes, after much debate we decided\n                            that an individual person can be a 'value-based group' if they are\n                            expressing values under their name, like religious scholars.\n                        </li>\n                        <li>\n                            <i>How about news organizations?</i> We ended up also letting news\n                            organizations be value-based groups, but only the opinion and op-ed\n                            sections where people are expressing their values.\n                        </li>\n                    </ul>\n\n                    <h3>Getting the Data Otherwise Known as our Entire Semester</h3>\n\n                    <p>\n                        You know that mantra that 90% of a data scientist's work is data\n                        wrangling/preprocessing? Well, I'd like to submit my Fall 2017 into\n                        evidence. I wrote more web scrappers this past semester than I\n                        thought I would in my entire life, but we'll get more into that in\n                        a bit. First, let's talk about how we even found data. Actually, we need to go back even further. How do we even decide what data we needed? Luckily for me, most of the work answering these questions\n                        fell to one of the other members of my team. This member identified\n                        various Arabic speaking groups throughout the globe who seemed\n                        to be large and influential enough to have some sort of website\n                        findable by Google. After identifying these groups, he scoured the\n                        web until he was able to find either official sites for these groups,\n                        or just sites that hosted their content. For example, he found a\n                        site that took newsletters ISIS released and upload those. In\n                        one case, another member of our groups found a University that\n                        had scrapped the dark web for a number of years and hosted a\n                        number of those results for free. We found one forum that fit out\n                        criteria and were able to use it.</p>\n\n                    <p>Then came the web scrapers. Ohhhhh the web scrapers. Given\n                        that I had the most background with programming, I was tasked\n                        with scraping all of the websites. I was given a list of URLs, and\n                        that's it. From there, I had free reign. Now, in order to scrape all\n                        of the blogs and forums in a scalable way, I researched for some\n                        pre-built scraping tools or libraries I could utilize. Now, don't get me\n                        wrong, there are a lot of mature and thorough tools for scraping\n                        websites, Scrapy being an especially good one, but they all ended\n                        up being useless for me. The reason for this is simple: they require\n                        the sites they scrape to be somewhat well written and predictable.\n                        Turns out, terrorists write shitty websites. Who knew?! I don't know\n                        if it's lack of technical prowess or just designing poorly fits in their\n                        evil brand, but the result is the same.  A series of poorly written sites\n                        that require hacked together, custom web scrappers to scrape. Ugh.</p>\n\n                    <p>I ended up going with the tried and true: requests and BeautifulSoup (and in one case, a library to bypass CloudFlare\n                        because terrorists really need DDoS protection). Basically, I\n                        had to look for patterns in the URL, or for embedded URLs in\n                        a webpage, and also some good markers to extract out the\n                        relevant text from the potpourri of text that BeautifulSoup\n                        pulls out. All in all, it was just a huge, tedious, pain in the\n                        butt. I spend about 10 hours over the course of a weekend,\n                        hold up in a library, doing nothing but writing and running\n                        web scrapers. </p>\n\n                    <p>I'm going to stop here for now. But please read on to part 2 to\n                        find out more, including how we preprocessed the data, feature engineering (or\n                        lack thereof), and some preliminary results.</p>\n                    ",
        "date": "2/6/2018",
        "location": "blog/2018/capstone1.html",
        "name": "Capstone Semester One Wrap Up (Part 1)"
    },
    {
        "content": "\n\n                    <p>Welcome back to the second part of the wrap up of the capstone work completed during my first semester\n                        of my Master's in Data Science Program. At this point, we have defined the problem and collected the data,\n                        so now it's time for the fun stuff.\n                    </p>\n\n                    <h3>To Preprocess, or not to Preprocess</h3>\n\n                    <p>In some ways, that isn't even the right question to ask. Of course\n                        we are going to have to preprocess, but how much and in what ways\n                        are key for us. We are touting these models as \"language agnostic,\"\n                        and the agnosticism must start at the preprocessing stage. So at\n                        this stage in our project, we have about 70,000 different documents,\n                        representing 20 different groups with an even split of violent and\n                        non-violent groups and at the end of the preprocessing pipeline, we\n                        should have a \"clean\" set of document that are ready to be feature\n                        engineered.</p>\n\n                    <p>Step one is quite clear: remove the junk. Junk is junk regardless\n                        of what language it is in so we can safely remove it. By junk, we\n                        mean anything that is not an Arabic character or numeric character.\n                        This was quite cleanly achieved through the use of regular expressions. There also becomes the question of how to deal\n                        with numbers. Numbers obviously signal something in a document,\n                        but exactly what is often unclear. And when all number are included,\n                        the feature space literally becomes infinite (in theory), but even in\n                        practice becomes quite large. Instead, we opt to give numbers special\n                        characters, namely, NUM. This really does finish out the preprocessing of individual documents.</p>\n\n                    <p>The next step is to determine what documents belong in a ready\n                        to go corpora for this research. Short documents (i.e, a couple of words), add computation time without significantly adding to the\n                        corpora since it is hard to express intent in only a couple of words.\n                        After looking at the word distribution of all documents, we find that\n                        the median number of words in a document is about 200, with the\n                        mean tending towards 100. The min is 0 words and the max is over\n                        50,000. After a fair bit of deliberation, we somewhat arbitrarily decided at a cutoff of 90 words for a document to be included in the\n                        corpus. After removing the runts of the group, we are left with about\n                        55,000 documents, more than enough to get us started.</p>\n\n                    <h3>The Features are Coming</h3>\n\n                    <p>Obviously, raw text is meaningless to any sort of machine learning model. One of the principal natural language processing (NLP) problems is how to convert\n                        from raw text to numbers that we can feed into our models. There\n                        have been great strides forward in NLP in areas such as sentiment\n                        analysis (how is the author feeling) and part-of-speech tagging\n                        (identifying the part of speech for any given word). Unfortunately,\n                        we cannot use any of that. Knowing enough about a language to\n                        identify author sentiment is far beyond that language-neutral approach that we seek. Thus, we have to go back the basics. The\n                        primary features extraction techniques that we use are TF-IDF\n                        weights and word embeddings.</p>\n\n                    <p>Without this post becoming an NLP lesson, I will touch on what\n                        these weights mean. TF-IDF (text frequency-inverse document frequency) weights map documents to feature vectors which attempt to quantify what words a document uses, how often it uses\n                        those words, and how effective those words are identifying the\n                        document. This last caveat ensures that common words, like \"the\" in\n                        English, aren't given high weights because even though they appear very often within any given document, they appear with the similar frequencies in almost every document, so they are ineffective at\n                        identifying any one document. Word embeddings are a very different\n                        approach. Instead of mapping documents to features vectors, they map words to feature vectors where the feature vector for any given\n                        word attempts to quantify the context with which that word appears. <i><strong>Caution, technical terms ahead.</strong></i> For\n                        the more technical reader who desires specifics, we actually used\n                        sublinear TF-IDF weights and the word2vec implementation of word embeddings. We trained our own word embeddings as opposed to\n                        using pre-trained since our use case is quite specific. The TF-IDF weights were calculated using Java and the word embeddings are a feature of the Keras deep learning library in Python. For the TF-IDF weights, we chose the top 10,000 features according to the information gain of the features.</p>\n\n                    <h3>Models on Models on Models</h3>\n\n                    <p>After months of work, we finally get to the easy part, taking all\n                        of the data and plugging it into our models to finally get some results! Because this is research and we don't know what models\n                        will perform best, we decided to use all of them. While I am obviously kidding, we did cast our net pretty wide in terms\n                        of models. We consider a number of baseline models like logistic\n                        regression, random forests, support vector machines, and gradient\n                        boosted trees. These models all take in the TF-IDF weights as features. Our \"novel\" models are deep learning models such as\n                        convolutional neural networks and LSTM networks. Even though these are relatively mature infrastructures, we consider them \"novel\" because a) they haven't been around nearly as long as the\n                        baselines and b) we expect them to outperform the baselines.</p>\n\n                    <p>The logistic regression was run using custom Java code. After\n                        this, we hit a bit of a problem. All of the other baseline models are\n                        very easily implemented in Python using the sklearn library, but the\n                        TF-IDF weights are calculated and exist in Java. We need to find a way to export these feature vectors and import them to Python.\n                        Now, these TF-IDF weights exist in a matrix (well, not exactly. They're stored as dictionaries, but close enough), so exporting them as a csv should be trivial. However, the space of it all coming into play. Each document is at least a 10,000 x 200 matrix containing double values and there are about 55,000 documents. You're looking\n                        at quite a few gigs of memory to store matrices that mostly contain\n                        0's (TF-IDF matrices are usually extremely sparse). Instead, we used\n                        a sparse matrix representation for the matrices. Specifically, each matrix was represented as a giant list of coordinates where each coordinate specified what value should exist at that coordinate.\n                        From here, we could have converted to binary files to save a little more space, but even without that, the exported feature files were\n                        only about a gigabyte total. Reading in these coordinate-value files\n                        is pretty simple using scipy's implementation of sparse matrices.</p>\n\n                    <p>With our features now over in Python, we can just plug and play\n                        the rest of our baseline models. For each, we did a grid search to help\n                        narrow down on some reasonable values for the hyperparameters. As for the neural networks, we used Keras with a Tensorflow backend\n                        to implement the networks. Regrettably, I have not had a huge part in the neural networks (at least not yet), as the original architectures\n                        were based on code from a past project and thus did not need a whole lot. We have a couple more architectures in mind that I would\n                        have the chance to be more involved in.</p>\n\n                    <h3>\"Houston, we have a (bias) problem\"</h3>\n\n                    <p>We are now almost in a position to talk about some preliminary results. We first need to briefly discuss our validation metrics. We\n                        had two different validation techniques. The first was a standard\n                        cross-validation technique and the second was a leave-one-group-out\n                        cross-validation (LOGO-CV) which involves leaving all documents from a single group out from training and use those documents\n                        for testing. We care more about the LOGO-CV results since they\n                        are more real to the application of this capstone. In terms of metrics,\n                        we looked at both accuracy and the F-1 score for both violent and non-violent groups. </p>\n\n                    <p> Now for the results. I won't go into exact values here since they will be listed out in the\n                        paper. The initial results were roughly as expected. The results of the\n                        standard cross-validation were ridiculously high, like almost 100%. Upon further investigation, we found out that this was because the\n                        model was actually picking up on a group's name in the learning phase so when it saw that group in test, it already knew the classification. The LOGO-CV results saw a pretty huge drop-off.\n                        For some groups, it was getting close to 100% still, and for others, it\n                        completely misclassified every document. All of the groups that it was getting 0% on were non-violent groups. What this means was that the models were essentially almost always guessing \"violent\".\n                        There are some ideas we have for mitigation of that will be included in the next section.</p>\n\n                    <p>These results were roughly consistent\n                        across models. Of the baseline models, logistic regression did the overall best. Overall, the two neural network models did the best,\n                        but the logistic regression was relatively competitive. These results\n                        are to be expected. Since neural networks are good at self-generating\n                        features, we expect them to perform well in situations where we\n                        only provide generic features. The logistic regression also isn't all\n                        that much of a surprise since logistic regression seems to just do\n                        at least ok at most tasks.</p>\n\n                    <h3>So, what's next?</h3>\n\n                    <p>That's approximately what we did during the first semester.\n                        so where do we go from here? The first thing that needs to be\n                        addressed, and something that we already have started to address\n                        as I write this is the imbalance in predictive accuracy between violent and non-violent groups. One of the root causes of this initially\n                        was one of our groups, Al-Boraq. This was the group whose documents we got from a research group that had collected forum\n                        posts for Al-Boraq. What this meant is that we have ~55,000 documents for Al-Boraq and ~15,000 documents for every other group. This is a very lopsided distribution that needs to be addressed.\n                        So the first step will be to downsample Al-Boraq randomly.</p>\n\n                    <p>Another step will be to do random oversampling in order to obtain perfectly even class distributions (i.e, the exact same number\n                        of violent and non-violent documents). Implementation-wise, this is a\n                        very simple change and will likely have a sizable effect. Another potential, but less feasible option, would be to diversify the corpus by\n                        getting documents from even more groups. Given the time-frame, this is unlikely to happen.</p>\n\n                    <p>We also may have to reconcile the fact that there is a fair bit of irreducible error in our data, where some documents are almost\n                        carbon copies of others but are labeled as both violent and non-violent. There are two obvious examples of this. Violent religious groups will often quote non-violent religious scholars in order\n                        to legitimize their cause. On the other hand, non-violent news\n                        organizations may post direct quotes from violent groups when\n                        reporting on them. It's hard to say how frequent these cases are,\n                        but they are almost impossible to rectify.</p>\n\n                    <p>Other than solving this issue, we have a few other ideas in mind\n                        for this semester. The first is to implement a few other neural network architectures, mainly GRUs and RCNN (recurrent convolutional neural networks) to see if they improve on the\n                        results of the other neural networks. Another really cool, but\n                        likely overly ambitious, idea would be to reverse engineer the\n                        CNN. This idea has been implemented with images, where you\n                        can see what inputs cause certain output neurons to fire by\n                        tracing the network backward. To our knowledge, this has not\n                        been done with text, so it may be too arduous.</p>\n\n                    <h3>Cool, but what have I learned?</h3>\n\n                    <p>After a grueling two blogs posts, we are almost there.\n                        But I would be remiss if I didn't go through a couple of things\n                        that I have learned so far. I'm more exausted writing this than you\n                        are reading, so I'm lazily just going to put these lessons in list form.\n                    <ul style=\"padding: 0em 5em\">\n                        <li>\n                            Research is a whole different beast from anything else that I have done, for better or (more often than not) worse. Doing any sort of\n                            research project comes with a couple of intrinsic evils that I have yet\n                            to come across in my academic career. The first is, as opposed to the nice, structured problems that you come across in classes, your research just might not work. You could spend weeks working on some part of your project to have it yield no benefit whatsoever.\n                            This is certainly something I will need to get used to because such\n                            things happen in industry all of the time, but it certainly is frustrating the first time you come across it. Also, the lack of\n                            specifications can be rather tumultuous. If you are working on a\n                            project for an end user, you can narrow in your specifications by\n                            prototyping and then interviewing the end user. In research, the end user\n                            is.........well, nobody. We have a sponsor but the overall end goal is to expand human knowledge in a certain domain. But how that is done\n                            is open-ended. The specifications are chosen by you and are altered\n                            based on what works and what doesn't. The ambiguity can be quite\n                            burdensome in terms of scheduling how long any given part of the\n                            project will take. If you can't tell, I think doing research has been an\n                            invaluable experience, but I don't think it's for me.\n                        </li>\n                        <li>\n                            Auditing is super important, especially as the project gets more complicated. Auditing is kind of the same as test cases in coding,\n                            but they are distinct because they can't really be automated or\n                            even anticipated. In coding, the testing pipeline is \"this is what\n                            my app needs to do, I can write some tests ahead of time to validate\n                            those features and then develop until the product can pass all of the\n                            tests.\" Such a pipeline does not translate to large research projects. You can, and should, write test cases for your code, but research, and\n                            data science in general, needs something else. Many times, you will perform some sort of analysis and get results, and the auditing is\n                            going in and making sure that your analytical procedure is valid\n                            and that your results are reasonable. This can be a tedious process\n                            because it should really be done after any bit of analysis, but can't\n                            really be automated. You just have to do it.\n                        </li>\n                    </ul>\n\n                    <p>That's it for now. I will make another post later on in the semester wrapping up\n                        the project, as well as linking our published paper.<p>\n                    ",
        "date": "2/10/2018",
        "location": "blog/2018/capstone1.html",
        "name": "Capstone Semester One Wrap Up (Part 2)"
    },
    {
        "content": "\n\n                    <p>This post is going to be a little bit different, and I hope this is a series that I can do more with. When most people think of data science, they think cutting edge deep learning and AI methodologies, and because of that it can seem a bit out of reach. People often forget that you can do some really fun stuff with data science using really simple, intuitive techniques. I want to do some posts on doing cool, data science-y things using simple concepts. This post is going to focus on generating text using a Markov Chain. This is inspired by the \"Random Kanye\" project, where people create models to generate random Kanye West tweets.</p>\n\n                    <p>I'm only going to briefly go over the statistics behind Markov Chains here, but for a really good background on this, listen to the \"Random Kanye\" episode of the <i>Linear Digressions</i> podcast (my current obsession). Basically, a Markov process is any process where the next state of the process is only dependent on the current state, not any of the past states. So, in the context of text generation, if I began the sentence \"I am from the United _____\", you could probably guess that the next word in that sentence is \"States\" and you could probably do it based only on the word \"United.\" Certainly, the other words help, but if I just said \"United _____\", you would probably still guess with reasonable certainty that the next word is \"States.\" This particular example displays the Markov property, since the next word depended only on the previous word, not any of the prior words. Now, a reasonable thing to point out would be that this doesn't always work. If I gave you the subset of a sentence, \"the _____\", you would be hardpressed to come up with the next word, since without context, almost any word can come after the word \"the.\"</p>\n\n                    <p>We can reconcile this by getting a large enough set of text and coming up with a probabilistic distribution for what words follow what other words. Essentially what this means is, scan over a large body of text looking at each pair of consecutive words, and keep track of how often words follow other words. So using the Random Kanye example from before, we might go through all his tweets and find out that the most common word that follows the word \"the\" is \"best\" (i.e, I am the best), while the next most common word is \"greatest\" (i.e, I am the greatest rapper). We would have a sense of how often each of these words follows the word \"the\". We would also have a sense of how often every other word in the vocabulary followed the world \"the\". Once we know how often every word in the vocabulary follows the world \"the\", we can easily turn this into a probability distribution by dividing by the total number of times we saw the word \"the\". So if we want to make a new Kanye tweet that starts with the word \"the\", we would just pick the next word in the tweet randomly accordingly to this probability distribution that we just defined. We would have a similar distribution for every word in the vocabulary, allowing us to probabilistically generate tweets from Kanye.</p>\n\n                    <p>Hopefully, this give you some sense of how we can use a Markov chain to generate new text from some corpus of training text. It turns out this works fairly well, and if you trained such a model on a large enough set of Kanye tweets, you would get decent results. It is important to note that since we are using Markov chains, these sentences have no memory. So while it often does an ok job, there is also a good chance that you just get incoherent rambling. Personally, I think these are the most fun because they are often hilarious. Now, the real fun in doing something like this is choosing your data source. In \"Random Kanye,\" the data source was Kanye tweets, but there is nothing limiting us to doing one sensible source. We can arbitrarily combine different data sources, and our model will treat them as one source, giving some pretty hilarious results.  For this post, we are going to combine the tweets of Donald Trump with the Bible. I'm not trying to be political in any way here, I just genuinely thought this combination would absurd. I am going to step through some of the code here, but by no means all of it. All of the code and data for this project is available <a href=\"https://github.com/bgreenawald/Markov-Text-Generation\" >here</a>.</p>\n\n                    <p>The first step is to get the data. I found a collection of about 30,000 <a href=\"https://github.com/mkearney/trumptweets\">Trump tweets</a> starting as long ago as 2009, and also found a text version of the <a href=\"http://www.gutenberg.org/ebooks/10>\">Guttenburg Bible</a>. I will largely omit the preprocessing details, but the pipeline was basically: remove any non-English characters, make all text lowercase, and replace endline characters will a special token. After this, I had a collection of preprocessed lines where each line was either a verse from Bible or a tweet from Donald Trump. I went ahead and pickled this list so I wouldn't have to keep redoing the preprocessing steps. For those unfamiliar, pickling is just Python's way of saving Python object for later use.</p>\n\n                    <p>Now that we have the data, the real fun can begin. First, we have to create the vocabulary. This just involves reading in every sentences, splitting on spaces, and saving all of the words to a list.</p>\n                    <pre class=\"prettyprint\"><code class=\"language-python\">\n# Create the vocabulary\nvocab = []\nfor line in text_full.split(\"\\n\"):\n    for word in line.split(\" \"):\n        vocab.append(word)\n        </code></pre>\n                    <p>This showed that the corpus had 1366285 total words, but after casting our list to a set (to remove duplicates), there were only 43714 unique words. Then, I create a mapping that maps every word to a number, and another that maps numbers to words.</p>\n                    <pre class=\"prettyprint\"><code class=\"language-python\">\n# Create a dict mapping vocab to index\nvocab_to_id = {}\nid_to_vocab = {}\nfor index, word in enumerate(vocab):\n    vocab_to_id[word] = index\n    id_to_vocab[index] = word\n                    </code></pre>\n                    <p>Finally, we can actually create our term frequency matrix. If <i>n</i> is the size of our vocabulary, this matrix is <i>n x n</i>, where each entry if how often the colum  word followed the row word. So if you found the number representing the word \"the\" (this number comes from the dictionary code above), and the number for the world \"president\" (call these number <i>x</i>, <i>y</i> respectivly), then the <i>x,y</i> entry of the term frequency matrix is how often the word \"president\" came after the word \"the.\" The code to make this matrix is below.</p>\n                    <pre class=\"prettyprint\"><code class=\"language-python\">\n# Fill up the tf matrix\nfor line in text_full.split(\"\\n\"):\n    words = line.split(\" \")\n    for i in range(len(words) - 1):\n        id1 = vocab_to_id[words[i]]\n        id2 = vocab_to_id[words[i + 1]]\n        tf[id1, id2] += 1\n                    </code></pre>\n                    <p>One useful implementation detail here is that your life will be a whole lot easier if at this point you convert the term frequency matrix to a sparse marix. Remember, this term frequency matrix is huge, but most of the entries are 0 since there are only so many valid pairing in the English language. Instead of making Python store this entire, mostly empty matrix, we can use a sparse matrix to save a ton of space. I personally use the scipy <i>coo</i> implementation, shown below. </p>\n                    <pre class=\"prettyprint\"><code class=\"language-python\">\nmat = sp.coo_matrix(tf)\n                    </code></pre>\n                    <p>Again, this is not required, but my relatively powerful laptop could not handle the code unless I did this, so it is effectively mandatory. </p>\n\n                    <p>A fun little aside, before we actually make new text, is to see what are the most common word pairings. I won't go through the code for this, but after finding the top 10 biggest elements matrix, I found that the 10 most common consecutive pairings of words are\n                    <ol style=\"padding: 0em 5em\">\n                        <li>of the</li>\n                        <li>the lord</li>\n                        <li>and the</li>\n                        <li>in the</li>\n                        <li>to the</li>\n                        <li>and he</li>\n                        <li>i will</li>\n                        <li>for the</li>\n                        <li>shall be</li>\n                        <li>all the</li>\n                    </ol>\n\n                    <p>As expected, these are pretty generic pairings of words. \"The lord\" is interesting because it is definitely Bible specific, but it is more common then even a pairing like \"and the\" that would appear in both the Bible and Trump tweets.</p>\n\n                    <p>Now, we are almost ready to actually make new text. The final step is to convert the term frequency matrix to a probability distribution (called a Markov or probability transition matrix). This is simply done by dividing every element in a row by the sum of the row.</p>\n                    <pre class=\"prettyprint\"><code class=\"language-python\">\n# Normalize the matrix\nmat_norm = normalize(mat, norm='l1', axis=1)\n                    </code></pre>\n\n                    <p>So now, we generate new text. To do this, we choose a start word, and using our probability transition matrix, keep adding words to the sentence until we hit an endline.</p>\n\n                    <pre class=\"prettyprint\"><code class=\"language-python\">\nsentence = start_word\nwhile start_word != \"endline\":\n    row_ind = vocab_to_id[start_word]\n    prob_dist = np.array(mat_norm.getrow(row_ind).todense())[0]\n    next_ind = np.random.choice(range(len(vocab)), p=prob_dist)\n    start_word = id_to_vocab[next_ind]\n    sentence += \" \" + start_word\n                    </code></pre>\n                    </p>\n\n                <p>Below are a couple of examples that I thought were fun. Using the code on Github, you can easily re-run this with whatever start word you desire.</p>\n\n                <strong>Start word: 'the'</strong>\n                <ul style=\"padding: 0em 5em\">\n                    <li>the nations shall many women working on the people when israel which are known loser who hath sent benaiah the white as i hope of macedonia and invent to the appearance they not nor taking over theaters this is mccarthyism</li>\n                    <li>the original costume was susan berry</li>\n                    <li>the midst of seventy years agothis is mad sometimes referred to be long massive tax just named eutychus being stubborn can call it</li>\n                    <li>the bar them that thou with his works nay but you have sinned against the lord</li>\n                </ul>\n\n                <strong>Start word: 'who'</strong>\n                <ul style=\"padding: 0em 5em\">\n                    <li>who escaped alone and treasuries shelemiah shemariah and moab and this country is the one came up he defrauded us is through phenice and thou shalt not hearken unto aaron the obamacare</li>\n                    <li>who was a son of them thou shalt thou art not the word again despite obamas terrible</li>\n                </ul>\n\n                <strong>Start word: 'hillary'</strong>\n                <ul style=\"padding: 0em 5em\">\n                    <li>hillary lying vanities but the goodness thou shalt hearken ye that were in all this day in twelve hours left to go way but of the lord of israel and her head</li>\n                </ul>\n\n                <p>So we can see a couple of interesting observations. The first is that because each word choice is random, we can use the same start word multiple times and get very different sentences. The second is that because this process has no memory, the sentence can start off very biblical and midway just switch to being quite Trumpian. This is the primary weakness for using Markov chains for text generation, but it's also why they are so much fun.</p>\n\n                <p>This wraps up this little diversion. Hopefully, this demonstrates how using very simple statistical techniques can do something extremely complex like text generation (although arguably it doesn't very well, but still). I encourage you to use the code on Github to try new start words to make sentences. The pipeline I made it also fairly generic, so it can accommodate any text you throw at it. The next combinations I intend to try Trump tweets and the Quron and Trump tweets and the Torah to see how swapping the religious text alters the results. Hope you enjoyed!<p>\n\n\n                ",
        "date": "3/18/2018",
        "location": "blog/2018/markov-text-gen.html",
        "name": "Fun with Simple Data Science: Markov Text Generation"
    },
    {
        "content": "\n\n                    <p>In the previous post, we explored how to use Markov chains to generate new text, and did so through the lens of combining to different corpora of text (Trump tweets and the Bible). In this post, which is going to be a short follow-up, we are going to address one fundamental issue with the original post: that there was no sense of memory in the model. </p>\n\n                    <p>By definition, Markov chains cannot have memory. As a brief review, the definition of a Markov process is one in which the next state of the system relies only on the current state, not on any of the past states. This presents a bit of an issue with text generation since human text is definitely not Markovian. In order to predict what a person is going to say next, you need at least a couple of words of context, usually closer to a full sentence. This issue was in full display in the sentences the model was creating. A sentence would often start in the tone of the Bible and then suddenly switch over to being Trumpian, or vice-versa. Take for examples \"who was a son of them thou shalt thou art not the word again despite obamas terrible.\" This entire sentence reads like a grammatically atrocious Bible verse up until the last two words, where somehow Obama gets thrown into the mix. To get more cohesion into these sentences, we need to introduce some form of memory.</p>\n\n                    <p>As stated, we really can't do this within the framework of a traditional Markov model, we're going to have to improvise a little. I should note that most of what's going on in this post was just me messing around with extending the model and doesn't have the same solid mathematical backing as the previous post. Nevertheless, let's just jump into it.</p>\n\n                    <p>The task at hand is to take the code used from the previous post and extend it to use not one, but <i>n</i> words in order to predict the next word. Now the data science savvy amongst you are immediately jumping to \"sequence to sequence RNN models.\" This is certainly the preferred method and would likely get us much better results, but I wanted to try something a bit different. Since this is \"Fun with Simple Data Science,\" I wanted to try and improve the results using only the concept of probability distributions introduced last time. Using an example, let's look at the ideal way to go about this problem. Keeping with the Trump tweet example, let's say we had the starter text \"I am the _____\" and want to predict the next word. To answer this, I would need to answer three questions: what words usually come 3 words after the word \"I\", what words usually come 2 words after the word \"am\", and what words usually come 1 word after the word \"the.\" I would combine the answers to these three subquestions and use that to generate my prediction. Here in lies a problem. The code we have can only look ahead one word. So we simplify the problem a little bit and just say, what words usually come after \"I\", \"am\", and \"the\" separately, and combine this information to generate the final prediction.</p>\n\n                    <p>With this methodology, we can now take any number of words as an initial input and just get the probability distribution for each word. Now the question becomes how do we combine <i>n</i> probability distributions into a single probability distribution. The answer is that we're going to do a weighted sum of the probability distributions. Formally, the final probability distribution <i>w</i> that we will use to generate our predictions is $$w = w_0 * word_0 + w_1 * word_1 + .... + w_n * word_n$$ The question of what weights to use is an interesting one and one that I don't necessarily have a good answer for. I chose to just test out a few different ones and see what gave the best results. </p>\n\n                    <p>I tested three different weighting schemes. The first was uniform weighting, where each word was given an equal vote when deciding the next word.</p>\n\n                    <pre class=\"prettyprint\"><code class=\"language-python\">\nweights = np.zeros(len(start_words)) + 1\nweights = weights / len(start_words)\n                    </code></pre>\n\n                    <p>The next was an exponential decay, where the most recent word was given the highest weight, and as you went further back, words got exponentially less of a vote.</p>\n\n                    <pre class=\"prettyprint\"><code class=\"language-python\">\nweights = [0] * len(start_words)\nfor i in range(len(start_words)):\n    weights[i] = math.pow(gamma, i)\n\nweights.reverse()\nweights = np.array(weights)/sum(weights)\n                    </code></pre>\n\n                    <p>Finally, I did a random weighting where weights were just chosen randomly.</p>\n\n                    <pre class=\"prettyprint\"><code class=\"language-python\">\nweights = np.array(random.sample(range(0,100), len(start_words)))\nweights = weights / sum(weights)\n                    </code></pre>\n\n                    <p>With the weights well defined, we can now start generating text. The code, shown below, is similar to how we generated text last time. We take in <i>n</i> words, and for each word get the probability distribution for that word and multiply it by the corresponding weight. Doing this for every word and summing the results gives us the probability distribution for the next word, which we predict and append to our result sentence. Now the word we just generated is used in the next prediction, along with the previous <i>n - 1</i> words. Continue this process until we hit an endline, and output that as the result.</p> \n\n                    <pre class=\"prettyprint\"><code class=\"language-python\">\n# Number of steps to look back\nlookback = len(start_words)\n\nwhile current_word != \"endline\":\n    # Get the last \"n\" words, where \"n\" is the lookback amount\n    lookback_words = start_words[-(lookback):]\n\n    # Start with the furthest back word and use it's distribution as the start\n    row_ind = vocab_to_id[start_words[-len(start_words)]]\n    prob_dist = weights[0] * np.array(mat_norm.getrow(row_ind).todense())[0]\n\n    # For all the rest of the words, add the weighted probability distribution to the result\n    for i, word in enumerate(lookback_words[1:]):\n        row_ind = vocab_to_id[word]\n        prob_dist += weights[i + 1] * np.array(mat_norm.getrow(row_ind).todense())[0]\n\n    # Make sure we don't have repeats\n    current_word = start_words[-1]\n    while current_word in lookback_words:\n        next_ind = np.random.choice(range(len(vocab)), p=prob_dist)\n        current_word = id_to_vocab[next_ind]\n\n    # Append the predicted word the results\n    start_words.append(current_word)\n                    </code></pre>\n\n                    <p>Now for the fun part. The results for a couple of different initial phrases are shown below. The results also compare each of the three weighting schemes.</p>\n\n                    <table class=\"alt\">\n                        <tr>\n                            <th>Starting Words</th>\n                            <th class=\"tg-l711\">Weights</th>\n                            <th class=\"tg-us36\">Generated Text</th>\n                        </tr>\n                        <tr>\n                            <td rowspan=\"6\">donald trump is</td>\n                            <td rowspan=\"2\">decaying</td>\n                            <td >donald trump is thinking an big guy leads to finally someone realize you for years the holy one that o trump house thou shalt j trump jeb used the wilderness a trump you believe</td>\n                        </tr>\n                        <tr>\n                            <td>donald trump is u the re tax their credit trump for</td>\n                        </tr>\n                        <tr>\n                            <td rowspan=\"2\">uniform</td>\n                            <td>donald trump is at the trump president j jimmy trumps son</td>\n                        </tr>\n                        <tr>\n                            <td>donald trump is we are ye might j trump when ill be trump only run the trump house youll j trump you your words money trump no shepherd he of the your arguing enemies shall be take great thx</td>\n                        </tr>\n                        <tr>\n                            <td rowspan=\"2\">random</td>\n                            <td>donald trump is have you i run donald trump international unlike anything worth trump</td>\n                        </tr>\n                        <tr>\n                            <td>donald trump is now hear the children pot of trump for</td>\n                        </tr>\n                        <tr>\n                            <td rowspan=\"6\">though shalt not</td>\n                            <td rowspan=\"2\">decaying</td>\n                            <td >though shalt not bless the king hand of illegals to bring ye another will draw</td>\n                        </tr>\n                        <tr>\n                            <td>though shalt not sincerely increase supposing the world they say again thou being wilt twelve men they cause some i of am the best interviews and delaying his the bramble and abigail the nabals wife looked bare all my lips soul</td>\n                        </tr>\n                        <tr>\n                            <td rowspan=\"2\">uniform</td>\n                            <td>though shalt not i in will he i is want to big we i too he turned a their way kings counsel</td>\n                        </tr>\n                        <tr>\n                            <td>though shalt not save thou shalt gleaned he they that thou me art and i will cast they sank i the have jesus remained u debate would have love they the every tea party i had prayed brought my he i that despiseth atlantic thou make livest these things ten list the on foxandfriends this great i feel say the i will save bless alive thee he the rested themselves what in greensboro your north it country charles he missouri hath trump you they a lacked mother opportunity in any case i disgraceful always we on actually the prophets and im love from</td>\n                        </tr>\n                        <tr>\n                            <td rowspan=\"2\">random</td>\n                            <td>though shalt not go use twitter trump</td>\n                        </tr>\n                        <tr>\n                            <td>though shalt not he then let us the freeing super up tuesday morning on television a it is not enough repeal signatures fools big league crowd thou shalt love roast with and an i emotional apprenticenbc still her daughters times fools in feedeth thy among thorns all thy the matters who dennis opposes nd choice worst ye thing say who unto them absalom</td>\n                        </tr>\n                        <tr>\n                            <td rowspan=\"6\">i am the</td>\n                            <td rowspan=\"2\">decaying</td>\n                            <td >i am the heavenly have forsaken covenanted with no a samaritan as that trust forsake ye begin to the ships asher and they went in out time gave an iraq will it appoint is you trump wins will written heal like neither can could only in truth not their healthcare would plan will be totally proved did are change bring back will i need your to hand pretending of not spare them look and thou he wonder of where to hear used the win will went and he testifieth thou disquieted shalt win</td>\n                        </tr>\n                        <tr>\n                            <td>i am the have young havent did was being offered merciful and man feareth have god and with thee o hosted by andrewejohnson</td>\n                        </tr>\n                        <tr>\n                            <td rowspan=\"2\">uniform</td>\n                            <td>i am the ephraimites best which was great</td>\n                        </tr>\n                        <tr>\n                            <td>i am the never have smote i them could not try cannot to get trump it am persuaded the law kindred for yourselves therefore said thus love congratulate have him a full of would a prenup</td>\n                        </tr>\n                        <tr>\n                            <td rowspan=\"2\">random</td>\n                            <td>i am the pray will say rebuke am will might say think always made for sat follow gave wonder will love have know just shall was see have just scatter will say looked gave have i said am testified therefore let have unto am may understanding hope have will hate say said daughter shall did have wish love cant let that say go may take am will should dont wrote rejoice will am sent thank will delivered came run should say will am tell his pray saw am will pray know cannot am have had love appointed was will agree find pray go loved completely am agree said had might truly see send will am watched have know am wonder cut will want have shall again would tell will vote need play will darius totally speak may beheld them hope will am</td>\n                        </tr>\n                        <tr>\n                            <td>i am the three will vote was able to shur and never have consulted with will also have to comprehend</td>\n                        </tr>\n                    </table>\n\n                    <p>We wrap up with a couple of observations. First off,  it seems that in general, this method produces longer phrases than the original model. My guess is that this is due to a much more noise in the final probability distributions, making it less likely to hit the \"endline\" character at any given time. Secondly, it does not appear that increasing the lookback really did anything to rectify the problem we were trying to solve, which was cohesion in our predictions. It still seems to be the case that the predictions are tonally and thematically all over the place. I attribute this to an overall small lookback and the fact that the algorithm we use doesn't really tackle the problem in the way that we would like. Nevertheless, any model that dictates \"though shalt not go use twitter trump\" can't be seen as too huge a failure.</p>\n\n                    <p>The code for this project can be found <a href=\"https://github.com/bgreenawald/Markov-Text-Generation\" >here</a>.</p>\n\n                    ",
        "date": "6/15/2018",
        "location": "blog/2018/markov-text-gen2.html",
        "name": "Fun with Simple Data Science: Markov Text Generation (Part 2)"
    },
    {
        "content": "\n\n                    <p>First things first, the Github URL for this project is <a href=\"https://github.com/bgreenawald/projmanr\">https://github.com/bgreenawald/projmanr</a> and for an overview of the project with screenshots, visit the <a href=\"../../content/software_developer.html\">software developer</a> page.</p>\n                    \n                    <h2>Background</h2>\n\n                    <p>I wanted to write a quick post talking about the history and current state of development for my R package, <i>projmanr</i>. <i>projmanr</i> started last summer as an independent study project for my Data Science Masters. I had opted out of an introduction to programming class and needed something to fill the space. Wanting to help sure up my R chops, I took on a project with a professor in the UVA Darden School of Business name Yael. Her idea was to create an R package to perform some of the simple tasks that a project manager might need (initially just calculating the critical path for a set of tasks and generating a Gantt chart). Even though I knew nothing of project management, she assured me that it would be easy enough to get up to speed and publishing an R package would be a great way to improve my R programming. </p>\n\n                    <p> At this point, you may be wondering \"why R?\". I wondered the same thing myself. Yael said that part of it was that R was her primary language of programming, but also that R is a growing choice for project managers (PMs) due to the fact that it is free and one of the more attainable languages for non-programmers. Everything sounded reasonable, so I was ready to dive in. Now we only had about 3 weeks until the official end of the independent study (this was a summer session), so out timetable was tight. The logic to calculate the critical path was straightforward and the creation of the Gantt chart used some modified code found online. But even though the two main features of the package were relatively simple, the actual creation of the package was not.</p>\n\n                    <p>The creation of this package definitely came complete with a few hurdles. The first was how to represent objects in R. For those unaccustomed to project management, the most granular data object is a `task`. A task is just a single part of a larger project. For the software devs out there, a task is really just a feature. A project is made up of a series of tasks, all of which have a duration and dependencies. The most logical way to programmatically represent a `task` was with an object. But here's the catch, R's implementation of objects is clunky at best. R natively supports S3 and S4 classes, but I found them extremely unintuitive to use. I decided to use the R6 library to represent my tasks. While still definitely not at the level of object-oriented languages, it did get the job done.</p>\n\n                    <p>The next big hurdle was changing my paradigm from `code that only I would see/use` to go that was actually going to be published. This meant, yes, actually writing robust tests. Now the CS department at UVA gave me more than enough background on how to write good tests. But the truth is, code that is written for college courses rarely actually gets the testing it deserves. Especially since much of my background is with data science, where most of the code you write is exploratory scripts that, let's be honest, often aren't tested at all. After hunkering down and knocking out the tests, I thought we had a pretty robust package (I later found out that one of the parameters in the primary function of the package didn't actually do anything. But hey, it's all a learning experience).</p>\n\n                    <p>The final hurdle was taking the code we had written and turning it into a proper R package. R packages are all published on the 'comprehensive R archive network,' or CRAN, and there are some very rigid sets of rules that must be followed to publish on CRAN. I highly recommend Hadley Wickham's \"R Packages: Organize, Test, Document, and Share Your Code\" for all you aspiring R devs out there. It is a free book that takes you through everything you need to know to create R packages. But even with the step by step directions from the book, publishing to CRAN is still no trivial task. When we finally thought we were submission ready, we still got rejected twice. The first time was because I messed up the licensing, and the second was because the reviewer disagreed with one of my adjective choices in the package description (I actually found the second rejection pretty amusing). At last, we finally got <i>projmanr</i> published to CRAN!</p>\n\n                    <h2>Current State of Development</h2>\n                    <p>Even though the independent study ended, we had many ideas to expand the package. We were even looking at adding some additional developers to help out. That being said, life got in the way. We both ended up having extremely busy semesters and school finally let out, I found out that Yael had taken a new job teaching up at Harvard! So that leaves just me working on the project.</p>\n\n                    <p> That being said, I'm happy to announce that I recently made the first major release for <i>projmanr</i> since it's initial publication (we had one minor release awhile back patching a few bugs). The biggest addition is the implementation of uncertainty for given tasks. In the initial release, you had to specify an exact value for how long each task would take (task 1 will take exactly 4 days). But this is highly unrealistic as things change in the real world all of the time! So now you can say specify a distribution over the end time for a given task. For example, I think the end time for this task will be normally distributed with a mean of 4 days and a standard deviation of 2 days. This allows you to run Monte Carlo simulations on the total end time for a project, given that some tasks are uncertain. This is actually a pretty exciting feature. Such functionality exists in most PM software, but we were able to use Rcpp (a c++ interface for R) so run the simulation extremely fast. We get substantial speed gains over `At Risk,` the Monte Carlo add-on for Excel. We also calculate the critical index for every task in the simulation. Finally, we added some increased customization options for the Gantt chart.</p>\n\n                    <h2>The Future of <i>projmanr</i></h2>\n\n                    <p>Now that we've covered the past and present of <i>projmanr</i>, what's next? Well, that's where I have hit a bit of a wall. Like I said, I know almost nothing about project management, so the development workflow up until now was to have Yael tell me something that PMs need to do, and then me find a way to implement it. But with Yael starting a new job, she will no longer be able to assist me. Further, once I find and start my new job, I will have less time to contribute to the project. So the future of <i>projmanr</i> comes as a call for help. I encourage PMs to download and use the package and post any ideas you have to the Github page (you can also contact me directly, but it's easier to just open an issue on Github to keep things organized). Further, if you are an R developer and want to contribute, there are already a couple of issues open on Github that you are welcome to take a whirl at. I have thoroughly enjoyed working on this package and hope that with the help of <i>projmanr</i> users, we can continue to grow it.</p>\n                    \n                    \n                    ",
        "date": "7/30/2018",
        "location": "blog/2018/projmanr.html",
        "name": "<i>projmanr</i> State of Development v0.2.1"
    },
    {
        "content": "\n                    <p>In this post, we're going to be talking through a potentially lesser-known data structure within Python, the humble set. This is going to be much more relevant to those approaching Python from a Data Science background as opposed to those with a Computer Science background. I have the luxury of both perspectives and can say that sets came up quite a bit in CS, but are not mentioned nearly enough in the Data Science community and I worry that many Data Scientists learning Python may overlook this extremely powerful tool. We'll start by identifying what a set is, mention a few use cases for a set, then compare the performance of sets versus other solutions.</p>\n\n                    <h2>What is a Set?</h2>\n                    \n                    <p>So first things first, what is a set? Sets are a mathematical construct arriving from discrete mathematics (I first ran into sets in my Discrete Math for Computer Scientists course). Sets are an <i>unordered</i> collection of <i>distinct</i> objects. So [1,2,3] is a set while [1,2,2,3] is not a set since '2' appears twice. Further, the set of [1,2,3] is equal to the set of [3,2,1] since elements are not ordered. Python supports two kinds of sets: 'set' and 'frozenset.' A 'frozenset' is simply the immutable version of the 'set.' We will be focusing just on the normal set today. Creating a set in Python is easy. Just pass in a list to the set function.</p>\n\n                    <pre class=\"prettyprint\"><code class=\"language-python\">\npython_set = set([1,2,3,3])\n# Will return [1,2,3]\n                    </code></pre>\n\n                    <p>It may seem that sets are somewhat niche given that they are unordered and without duplicates. In data science we often find ourselves dealing with collections that do have some underlying order and will indeed have duplicates. But you will find that sets arise more often then you would think, especially in exploratory analysis. Just last week I found myself needing to know if a large group of user_ids contained any duplicates (in fact, this is what inspired me to write this post). There are certainly many ways to approach this sort of problem. A naive approach might be \"iterate over the list and add each id to a new list, checking before each insertion if the element is already in the new list. This would look something like</p>\n\n                    <pre class=\"prettyprint\"><code class=\"language-python\">\nnew_list = []\nfor id in user_ids:\n\tif id in new_list:\n\t\treturn False\n\tnew_list.append(id)\nreturn True\n                    </code></pre>\n\n                    <p>However, using sets, we can solve this same problem elegantly (and as we'll see later, much faster) in one line</p>\n\n                    <pre class=\"prettyprint\"><code class=\"language-python\">\nreturn len(new_list) == len(set(new_list))\n                    </code></pre>\n\n                    <p>Now those familiar with the data science Python stack may have come across the numpy solution to this problem, the 'unique' method. That would look something like </p>\n\n                    <pre class=\"prettyprint\"><code class=\"language-python\">\nreturn len(new_list) == len(numpy.unique(new_list))\n                    </code></pre>\n                    \n                    <h2>Set Performance</h2>\n                    \n                    <p>The inevitable next question is why you would use sets. The big draw to sets is their performance. Let's consider two use cases for sets: creating a list of unique elements and seeing if an element is contained within a collection. Starting with creating a list of unique elements, we consider a couple of different methods for attaining this. For all methods, we start by using numpy to create a list of integers with duplicates using</p>\n\n                    <pre class=\"prettyprint\"><code class=\"language-python\">\nnum_list = np.random.choice(1000, x)\n                    </code></pre>\n                    \n                    <p>This creates a list of 'x' integers in [0, 1,000). 'x' is going to vary to demonstrate different sized list on performance. We will again start with the naive method of adding each element to a list, one at a time, after checking that the list does not already contain the element. </p>\n                    \n                                        <pre class=\"prettyprint\"><code class=\"language-python\">\nno_dups = []\nfor num in num_list:\n    if num not in no_dups:\n        no_dups.append(num)\n                    </code></pre>\n                    \n                    <p>Next, we will add all elements to a new list, maintaining duplicates. We will then transform this new collection to a set.</p>\n                    \n                    <pre class=\"prettyprint\"><code class=\"language-python\">\nno_dups = []\nfor num in num_list:\n    no_dups.append(num)\nno_dups = set(no_dups)\n                    </code></pre>\n                    \n                    <p>The reason we add each element to a new list is to make the test more consistent with the naive method. Finally, we test the numpy version.</p>\n                    \n                    <pre class=\"prettyprint\"><code class=\"language-python\">\nno_dups = []\nfor num in num_list:\n    no_dups.append(num)\nno_dups = np.unique(no_dups)\n                    </code></pre>\n                    \n                    <p>Using the 'timeit' library in Python, we run each of these methods ten times for varying sizes of list and track the total amount of time taken. The results are shown in the table below (time in seconds).</p>\n                    \n                    <table>\n                        <tr>\n                            <th>Size of List</th>\n                            <th>Naive</th>\n                            <th>Set</th>\n                            <th>Numpy</th>\n                        </tr>\n                        <tr>\n                            <td >1000</td>\n                            <td >0.0425</td>\n                            <td class=\"tg-031e\">0.0013</td>\n                            <td class=\"tg-yw4l\">0.0102</td>\n                        </tr>\n                        <tr>\n                            <td class=\"tg-031e\">10000</td>\n                            <td class=\"tg-031e\">0.756</td>\n                            <td class=\"tg-031e\">0.0128</td>\n                            <td class=\"tg-yw4l\">0.0211</td>\n                        </tr>\n                        <tr>\n                            <td class=\"tg-031e\">50000</td>\n                            <td class=\"tg-031e\">3.9523</td>\n                            <td class=\"tg-031e\">0.0748</td>\n                            <td class=\"tg-yw4l\">0.104</td>\n                        </tr>\n                        <tr>\n                            <td class=\"tg-031e\">100000</td>\n                            <td class=\"tg-031e\">8.0989</td>\n                            <td class=\"tg-031e\">0.1481</td>\n                            <td class=\"tg-yw4l\">0.2109</td>\n                        </tr>\n                        <tr>\n                            <td class=\"tg-031e\">500000</td>\n                            <td class=\"tg-031e\">40.3273</td>\n                            <td class=\"tg-031e\">0.82735</td>\n                            <td class=\"tg-yw4l\">1.1591</td>\n                        </tr>\n                        <tr>\n                            <td class=\"tg-031e\">1000000</td>\n                            <td class=\"tg-031e\">83.1135</td>\n                            <td class=\"tg-031e\">1.8269</td>\n                            <td class=\"tg-yw4l\">2.5075</td>\n                        </tr>\n                    </table>\n\n                    <p>One thing is immediately apparent: the naive method stinks. As the size of the list gets larger, sets blow them out of the water. Generally, numpy performs comparably, but always slightly worse. My guess is that under the hood, numpy is using sets, but the additional layer of abstraction causes a slight performance hit.</p>\n                    \n                    <p>The next task is seeing if a given element is part of a collection. There are many cases that need solutions to this task. For example, seeing an email has already been registered given a large list of registered emails. To test this, we will create a list of the numbers between 0 and 10,000, counting by 5. Then we will randomly generate numbers in the range 0 to 100,000 and one by one test if they are in the set. First, generate the list of numbers. We will be testing the 'list' object versus the 'set' object versus the 'numpy' array. It is import to note that they will all be identical in terms of contents.</p>\n\n                    <pre class=\"prettyprint\"><code class=\"language-python\">\nnum_list = range(100000)\nnum_list_set = set(num_list) \nnum_list_numpy = numpy.unique(num_list)\n                    </code></pre>\n                    \n                    <p>We generate a list of random numbers of varying sizes using</p>\n                    \n                    <pre class=\"prettyprint\"><code class=\"language-python\">\ntest_list = np.random.choice(100000, x)\n                    </code></pre>\n                    \n                    <p>Which will create a list of size 'x' containing random integers in [0, 100000]. For each list  'cur_list' defined above, we will test the inclusion of each element in 'test_list' using </p>\n                    \n                    <pre class=\"prettyprint\"><code class=\"language-python\">\nfor elem in test_list:\n\tif elem in cur_list:\n\t\tpass\n                    </code></pre>\n                    \n                    <p>We don't actually want to do anything with the results, we just want the inclusion checked, so we put a 'pass' inside the conditional. We will run each 5 times to get a more consistent measure. The results are shown in the table below.</p>\n\n                    \n                    <table class=\"tg\">\n                        <tr>\n                            <th class=\"tg-031e\"></th>\n                            <th class=\"tg-031e\">Normal</th>\n                            <th class=\"tg-031e\">Set</th>\n                            <th class=\"tg-031e\">Numpy</th>\n                        </tr>\n                        <tr>\n                            <td class=\"tg-031e\">1000</td>\n                            <td class=\"tg-031e\">1.191</td>\n                            <td class=\"tg-031e\">0.0003</td>\n                            <td class=\"tg-031e\">0.0216</td>\n                        </tr>\n                        <tr>\n                            <td class=\"tg-031e\">5000</td>\n                            <td class=\"tg-031e\">6.3247</td>\n                            <td class=\"tg-031e\">0.003</td>\n                            <td class=\"tg-031e\">0.1167</td>\n                        </tr>\n                        <tr>\n                            <td class=\"tg-031e\">10000</td>\n                            <td class=\"tg-031e\">13.1409</td>\n                            <td class=\"tg-031e\">0.0058</td>\n                            <td class=\"tg-031e\">0.2513</td>\n                        </tr>\n                    </table>\n                    \n                                        \n                    <p>The results here are even more lopsided than before. The regular list does terribly, the numpy array does comparatively better, but the set is the clear winner.</p>\n                    \n                    <h2>Conclusion</h2>\n                    \n                    <p>In this post, we learned what sets were, discussed a few ways that they can come up in everyday programming, and showed that, when applicable, sets blew regular lists and numpy arrays out of the water in terms of performance. And the beauty is that converting from lists or numpy arrays to sets is trivial. I hope I have laid out a compelling argument for sets. They are an easy and elegant way to heavily boost Python performance in certain situations.</p>\n                    \n                    ",
        "date": "7/23/2018",
        "location": "blog/2018/python-sets.html",
        "name": "Why You Should Really Be Using Sets (Python Tips)"
    },
    {
        "content": "\n                    <p>For me, the most enticing aspect of coding it the ability the build things that make your, or someone else's life easier. Sometimes I get lost in the minutiae of making my code beautiful and efficient that I forget that coding is really about making stuff and solving problems (even silly ones). My most recent foray into this lesson involved the simple grocery list, and my lack of desire to make one every week. So, naturally, I decided to make an app for my phone that auto-generates one for me. But to understand how we got here, we have to go back a little.</p>\n\n                    <p>Like most people in their early twenties, I recently had a somatic reckoning, realizing that my metabolism had humbled substantially since my teenage years. Consequently, I could no longer eat with no regard and still have my body look like I wanted it to. So I began meal prepping every week. I chose a couple of healthy meals that I actually wanted to eat, and would make them for the whole week. After a satisfactory trial period, and a fair bit of tweaking, I decided to make this my long term nutrition plan.</p>\n\n                    <p>Since I now had a very regular diet, I decided to do something I've long been curious about: find out exactly what my daily nutrition looks like. Specifically, how many calories do I ingest daily, and what is my daily distribution of macros? In order to do this, I sat myself down one weekend and listed out all the ingredients that go into my new diet and found out the nutrition information for each one. After an hour or so of tedious Google Sheets data entry and aggregation, I had a complete spreadsheet of my daily nutritional activity.</p>\n\n                    <p>While this was interesting in and of itself, I immediately saw greater potential for this spreadsheet. I now had a complete list of my weekly food needs, which is not far from a grocery list. Simply adding a boolean column that indicated whether I needed a given ingredient on a given week would essentially give me my grocery list. This way, I could determine my grocery list each week in a few short minutes.</p>\n\n                    <p>In hindsight, I could've stopped the project right there. It would be easy enough to just pull the spreadsheet up on my phone and use that as my actual list. <i>But that's not fun at all.</i> No, I wanted to autogenerate this into a proper grocery list. I turned to a list app I already frequently use, Google Keep, as my new go-to grocery store app. At this point, my goal was simply to write a program (in Python, of course) that would read in from my Google Sheet, figure out what ingredients I needed based on my 'Need' column, and write those items to a Google Keep Note.</p>\n\n                    <p>Google has some notoriously good APIs for many of their products, including Sheets. After getting a set of API credentials, I turned to the fantastic Python library <i>gspread</i>, which made reading my Sheet into a <i>pandas</i> dataframe relatively pain-free. Some trivial filtering generated the list of items I needed each week. The writing to a Keep note ended up being nearly as pain-free. While Google does not support an official API for Keep, the Python library <i>gkeepapi</i> worked splendidly. Following the examples in the documentation made developing the desired functionality a breeze.</p>\n\n                    <p>This really should've been where the project ended. Throughout the week, I would update the Sheet as I ran out of ingredients, and before I went to the grocery store on the weekend, I would run my program and my completed list would be accessible in my phone via the Keep app. But just a couple week later, I ran into my first hiccup. I decided to do a spontaneous grocery store run after work instead of over the weekend. This meant I didn't have access to my personal computer to execute my program. I could've easily just accepted this one edge case, but no. As a 21st human being, I demand that my grocery list be autogenerated no matter what the circumstance. I had no choice, I had to turn this into a mobile app.</p>\n\n                    <p>In theory, this new objective should've been just as easy as the other parts. I have some background in Android development, and this app only had to be a single button that I could push to generate my list. I already knew the basic architecture for my app. I would simply package up my program and deploy it as a Lambda function on AWS. Upon pushing the generate button in my app, the lambda function would be called and I would never have to manually create a grocery list again.</p>\n\n                    <p>Alas, this part of the project ended up taking disproportionately long. It ended up shining a spotlight on one of my biggest flaws: sometimes I get so fixated on how I think things should be that I can waste an exorbitant amount of time forcing them to be that way, rather than stepping back and seeing if there is a simpler the solution. The problem at hand dealt with the execution of the Lambda function from my app. Basically, I really wanted Java and Android to work as cleanly with AWS as Python. In Python, using the <i>boto3</i> library, you can just create a Lambda client and execute Lambda functions to your heart's content in a single line of code. I <i>realllyyyy</i> thought it would be this easy with Java and Android. To my discontent, after testing out various versions of both the AWS Java SDK and the AWS mobile SDK, and crashing my Gradle build more time than I cared to admit, I was lost. There just did not seem to be a solution as clean and intuitive as Python, and no amount of force was going to make it that way. Hell, by the end of this rabbit hole, I was exploring the AWS method for giving a mobile device temporary credentials to access cloud resources, just to execute my damn grocery list Lambda function.</p>\n\n                    <p>This one small roadblock almost made me nix this part of the project. But like most people who develop as a hobby, I'm nothing if not persistent. I began thinking about other AWS services that could trigger my Lambda function. Maybe I could have my Lambda subscribed to an SNS topic that my app could push to. Or maybe I could create some sort of queue with SQS. But, like most issues that arise in development projects, there was a nearly trivial solution waiting to be found. In this instance, that solution was API Gateway. I had somehow forgotten that one of the most popular triggers for Lambda functions were requests sent to API Gateway. And if there's one thing every modern programming language can do well, its send HTTP requests to an API. After discovering this, I got to embrace the immensely enjoyable experience of resetting my Android project, knowing that this time I wouldn't have to touch the Gradle configuration at all. And after that reset, everything fell into place.</p>\n\n                    <p>My final architecture for this project ended up as follows. I start off going into my Google Sheets sheet and updating my grocery list for the week by just putting a 1 or 0 beside each ingredient. Then, I go to an Android app on my phone that has a single button. Upon pushing that button, the app sends an asynchronous HTTP request to API Gateway. API Gateway triggers the Lambda function containing all the application code. That code reads from my Google Sheet and filters the ingredients to get only those that I need. The code then creates a list in Google Keep and populates it with my ingredients. Upon completion, the API Gateway returns a success code to the app, which displays a little message. And then I can access my new grocery list from anywhere.</p>\n\n                    <p>Before wrapping up, let me just list out a few more gotchas I ran into in implementing this project.</p>\n        \n                    <ul style=\"padding: 0em 5em\">\n                        <li>This is an obvious issue looking back, but it took me a while to figure it out since AWS didn't give the most descriptive error message. The packaged function for the project ended up being pretty large, so I decided to follow the best practice and send it to S3 and have Lambda read from there. However, I kept running into an issue where I would try and associate my upload code to Lambda and it would fail, saying something along the lines of 'Lambda can't find your main method'. What this ended up actually being was that Lambda didn't have access to the bucket containing my code. This can be resolved either by making the bucket public (not recommended since the code contains some credentials) or giving the IAM role associated with the Lambda function read only access to S3 (I chose the latter.</li>\n\n                        <li>I use a variety of phone cleaning apps, which remove junk files from your phone, to keep my phone running smoothly. However, since my app isn't an official app, it kept getting targeted by the cleaning apps for deletion. So when deploying custom Android apps on your phone, make sure any phone cleaner's you used don't delete them.</li>\n\n                        <li>If you ever find yourself in a situation where you've been working out of a private repo, but want to make the repo public and you need to remove sensitive data from your repo history, look into <i>BFG Repo Cleaner</i>.</li>\n                    </ul>\n\n                    <p>All in all, this ended up just being a fun little project to help automate one tedious aspect of my life. It ultimately helped demonstrate that coding has application <i>everywhere</i> and that projects are rarely ever done. There's always more tweaks and adjustments you can add to make the project more awesome. As always, the code is available on <a href=\"https://github.com/bgreenawald/Grocery-List-Generator\">Github</a>.</p>\n\n                    ",
        "date": "2/9/2019",
        "location": "blog/2019/grocery-list.html",
        "name": "Grocery List Generator"
    },
    {
        "content": "\n\n                    <h3>Introduction</h3>\n\n                    <p>\n                        I'm very excited to be launching the project I've been working on over the summer, Natural Language Playground (NLPLayground). Many of the personal projects I work on tend to revolve around NLP in some way and I've long wished to make the models available for users to interact with live. NLPlayground is the realization of that goal, making live versions of a variety of models and algorithms publically available. As noted on the homepage of the application, many of the models and algorithms here are not original and I have done my best to cite my inspirations where ever I can. However, these are my implementation of these processes and making them live in this way is not something that I have seen before.\n                    </p>\n\n                    <h3>Projects</h3>\n\n                    <p>\n                        At launch, the NLPlayground has three primary projects. Currently, I have no plans to add other projects, but rather to add content to the existing ones. However, now that I have the infrastructure in place to make these projects available, I will be on the lookout for new things to add.\n                    </p>\n\n                    <p>\n                        The first of the three is a MadGab-er. For those unaware, Madgab is a game where users are presented with common phrases that have been \"MadGab\"-ed and need to guess the original phrase. MadGab is based on mondegreens, which are, according to Google, \"misunderstood or misinterpreted words or phrase resulting from a mishearing of the lyrics of a song.\" A Madgab clue is a collection of words that when said in the right way, actually sound like something else. For example, the phrase \"Wander Her Womb Hen,\" when said in the right way, sounds like \"Wonder Women.\"\n                    </p>\n\n                    <p>\n                        It's a very fun game and ever since playing it, I've always thought it would be cool to have an algorithm that can take any phrase and turn it into a MadGab. The algorithm I present isn't exactly that, however it is close enough to get the same effect. I call it RadGab. Whereas in MadGab the clues are real English words, RadGab clues are gibber-ish that when said in the right way, sound like an English phrase. I will write a full blog post soon on the algorithm that I have come up with and I am also working with a friend on a full-fledged online version of the game, which should be live also be live soon.\n                    </p>\n\n                    <p>\n                        The second of the three projects is Markovian Mix and Mash. This is my implementation of the <a href=\"https://genesisofkanye.tumblr.com/\">Random Kanye</a> project, which used a Markov chain to mash together Kanye West streets and Bible verses. I already have a two (<a href=\"https://bgreenawald.github.io/blog/2018/markov-text-gen.html\">1</a>, <a href=\"https://bgreenawald.github.io/blog/2018/markov-text-gen2.html\">2</a>) full blogs post up about the project, so I will not go into any details here. The project that is already live allows you to mix Trump tweets and Bible verses. It is fairly easy for me to add new models (note, it does not have to be only two data sets, I can mash together an arbitrary amount) so I am certainly open to suggestions!\n                    </p>\n\n                    <p>\n                        The final project is Neural Network (NN) text generation. This project uses an <a href=\"https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\">implementation</a> of the char-rnn developed by Andrew Karpathy based on his famous post, <a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\">\"The Unreasonable Effectiveness of Recurrent Neural Networks.\"</a> It allows for the generation of new text based on a set of input data. For example, one of the models can generate new boy names based on a set of 1000 popular boy names. At launch, the available models will be boy names, girl names, dinosaur names, and Pokemon names. Again, it is fairly easy for me to add new models given a homogenous set of data with at least 1000 observations, so ideas for new ones are welcome!\n                    </p>\n\n                    <p>\n                        The code for each of the individual projects is open source and available on my <a href=\"https://github.com/bgreenawald\">Github</a>.\n                    </p>\n\n                    <h3>Architecture</h3>\n\n                    <p>\n                        For the technically inclined, this section will go over the architecture used for this project. I knew from the start I wanted a Lambda based architecture to minimize my hosting costs for the project. Initially, I wanted to go completely serverless and use the browser SDK for AWS to call my Lambda functions directly. But after trying a couple of different methods, this became too much of a security risk. I would either have to include API keys in the client code or mess around with user authentication, which seemed like overkill for this project.\n                    </p>\n\n                    <p>\n                        I then transitioned to just use a Flask backend to call all of my Lambdas to avoid any security risks and not have to mess with authentication. I also found that you can host a Flask project on Heroku virtually for free. Since the backend need only make API calls and doesn't have to do any actual computation on its own, this should be a robust enough solution assuming the site doesn't get too popular. This also allowed me to utilize jinja2 templates, making the process of adding new projects very easy. The front end just used a premade template from HTML5 UP.\n                    </p>\n\n                    <p>\n                        I had the site done with this architecture and nearly ready to go before realizing that I had 2 fairly major design errors. The first was that Flask is a synchronous framework and since my Lambda function calls take a little bit of time, I realized I was quickly going to get some performance throttling, since the number of concurrent users I could reasonably handle was equal to the number of cores in my host CPU. I decided to tackle this in two ways. The first was to use Lambda warming to minimize the risk of cold starts for my functions. This entailed having another Lambda function which called all three of the other Lambda functions every 15 minutes. The second was to convert my Flask framework to Quart, an asynchronous version of Flask. Despite causing a slight increase in price to keep the Lambdas warm and a slight increase in difficulty deploying Quart, these changes were necessary to ensure consistent performance of the application. The application is deployed using Heroku.\n                    </p>\n\n                    <h3>Conclusions</h3>\n\n                    <p>\n                        I am extremely excited to get this project launched and for people to get to interact with NLP in a (hopefully) fun way. As for the future, I intend to keep adding to the existing models as I get ideas and/or suggestions. If new projects come along, those will be added as well. So please reach out with any comments or suggestions, and enjoy the NLPlayground!\n                    </p>\n\n                    ",
        "date": "9/8/2019",
        "location": "blog/2019/nlplayground.html",
        "name": "NLPlayground"
    }
]