[
    {
        "content": "\n\t\t\t\t\t\t\t<p>Here we are, my very first blog post. The main objective of\n\t\t\t\t\t\t\tthis post is to give a little taste of who I am as a person\n\t\t\t\t\t\t\tand also motivate the existence of this site.</p>\n\n\t\t\t\t\t\t\t<p>So who am I? Well, at risk of plagiarising my own Linked-In profile, I am a graduate student at the University of Virginia\n\t\t\t\t\t\t\tcurrently pursuing my Masters in Data Science. I chose to pursue this degree as I saw Data Science as the most cutting-edge manifestation of the subjects I studied as an undergrad\n\t\t\t\t\t\t\t(I double majored in Computer Science and Mathematics with a concentration in Probability and Statistics, also at the University of Virginia). I will complete\n\t\t\t\t\t\t\tthis degree in May 2018, so I am also going through the trials\n\t\t\t\t\t\t\tand tribulations of a first-time job seeker. That being said, I do\n\t\t\t\t\t\t\tnot want to get into too much more detail about myself here,\n\t\t\t\t\t\t\tbecause that is what this site is about! Exploring it should give\n\t\t\t\t\t\t\tyou a relatively complete picture of who I am and what I am\n\t\t\t\t\t\t\tall about.</p>\n\n\t\t\t\t\t\t\t<p>So why exactly am I making a site for myself? I've listed\n\t\t\t\t\t\t\ta few compelling reasons on the homepage of this site,\n\t\t\t\t\t\t\tbut really the motivation for this site came from enough\n\t\t\t\t\t\t\tpeople telling me that it would be a good idea to have\n\t\t\t\t\t\t\ta personal site. I put off making one for various reasons,\n\t\t\t\t\t\t\tbut once I discovered that I already used (Github), had\n\t\t\t\t\t\t\ta mechanism for hosting static websites for free (Github Pages),\n\t\t\t\t\t\t\tI was officially out of excuses. I can think of a few additional reasons beyond what has already been listed. I think the exercise of publishing content could very well change the way\n\t\t\t\t\t\t\tI interact with the world. Whenever I am undergoing some\n\t\t\t\t\t\t\tinteresting or challenging problem in my life, I can view it through the lens of something that can be written up and\n\t\t\t\t\t\t\tposted to the world. I imagine that this will\n\t\t\t\t\t\t\thelp me engage with the world around me. Publishing is also\n\t\t\t\t\t\t\ta good mechanism to help hold myself accountable. My next\n\t\t\t\t\t\t\tblog post is going to involve my 2017-2018 New Years resolutions and the hope is that putting them out in a more\n\t\t\t\t\t\t\tpermanent way online will help me to actually remember and\n\t\t\t\t\t\t\tfollow through on them.</p>\n\n\t\t\t\t\t\t\t<p>So what exactly do I want this site to be? At the end of the day,\n\t\t\t\t\t\t\tI just it to be the most complete digital picture of myself out\n\t\t\t\t\t\t\tthere, and one that I have complete and total control over.\n\t\t\t\t\t\t\tBeyond that, I do not know what this site could become. Most\n\t\t\t\t\t\t\tnotably, I do not want to set any sort of goal or theme for this\n\t\t\t\t\t\t\tblog. It will certainly contain updates on my Capstone research\n\t\t\t\t\t\t\tand experience as a first-time job hunter, but I do not want to\n\t\t\t\t\t\t\tlimit myself to those things. This is really an outlet to share\n\t\t\t\t\t\t\tanything in my life that I think others might find useful.</p>\n\n\t\t\t\t\t\t\t<p>Will that in mind, I hope that you will explore the rest of my site and I can't wait to see how this project evolves!</p>\n\t\t\t\t\t\t\t",
        "date": "12/23/2017",
        "location": "blog/2017/introduction.html",
        "name": "Introduction Blog"
    },
    {
        "content": "\n\t\t\t\t\t\t<p>I'm going to be honest here, I am not the biggest fan of New\n\t\t\t\t\t\tYears Resolutions. Don't get me wrong, I think the idea of\n\t\t\t\t\t\tidentifying areas for self-improvement and working\n\t\t\t\t\t\ttowards those goals is great, I just think waiting for\n\t\t\t\t\t\ta calendar to tell you \"it's time to improve yourself\"\n\t\t\t\t\t\tis a bit silly. However, I do think the New Years at least\n\t\t\t\t\t\tprovides a time to sit back and reflect, especially since\n\t\t\t\t\t\tit is around this time that people, myself included, have\n\t\t\t\t\t\tsome time off and therefore can spend time reflecting.\n\t\t\t\t\t\tI've decided this year to publish my New Years Resolutions,\n\t\t\t\t\t\tmostly so I have a tangible and public set of goals, so\n\t\t\t\t\t\tanyone that reads this and knows me can hold me accountable to these resolutions. Also, I frankly do not remember what\n\t\t\t\t\t\tmy resolutions last year were, so having them easily accessible\n\t\t\t\t\t\tshould increase my chances of actually doing them.</p>\n\n\t\t\t\t\t\t<p>Without further ado, let's jump into my New Years Resolutions for 2018.<p>\n\t\t\t\t\t\t<ol>\n\t\t\t\t\t\t<li> <strong>Consume less, create more.</strong> I do not\n\t\t\t\t\t\tconsider myself a procrastinator. In fact, I am pretty good\n\t\t\t\t\t\tat getting things done well before they need to be done.\n\t\t\t\t\t\tHowever, I am quite adept at wasting time. Too much of\n\t\t\t\t\t\tmy day is spent consuming content that adds no real\n\t\t\t\t\t\tvalue to my life. Youtube and Netflix are the worst offenders here, with\n\t\t\t\t\t\tFacebook and Snapchat being close runner-ups. A simple\n\t\t\t\t\t\tway to achieve this goal would be to remove myself from\n\t\t\t\t\t\tthose platforms, but I do not see that as realistic. There are\n\t\t\t\t\t\tvery good things those platforms have to offer, I just need\n\t\t\t\t\t\tto be better at practicing moderation. More tangibly,\n\t\t\t\t\t\tI need to not use them just as a means to kill time. What\n\t\t\t\t\t\tdo I plan on doing with all the time I'll be getting back?\n\t\t\t\t\t\tWell, create more, whatever that may look like. This\n\t\t\t\t\t\twebsite is a great start, so maintaining and growing it\n\t\t\t\t\t\tout will be a good way to work towards this goal. Getting back\n\t\t\t\t\t\tinto writing music, and maybe this time actually record some\n\t\t\t\t\t\tof it, will also be valuable in achieving this goal.\n\t\t\t\t\t\tContinuing to work on the <i>projmanr</i> R package and\n\t\t\t\t\t\t<i>Super Smash Bros. SE</i> game mod will also be productive as well.</li>\n\n\t\t\t\t\t\t<li><strong>Invest more in people.</strong> I admit it,\n\t\t\t\t\t\tI can be a bit of a workaholic. For easily the past decade\n\t\t\t\t\t\tof my life, school has been the most important aspect\n\t\t\t\t\t\tof my life, especially my last four years in college.\n\t\t\t\t\t\tThis emphasis on my education has really led me\n\t\t\t\t\t\tto neglect the people in my life, especially my group\n\t\t\t\t\t\tof friends.\n\t\t\t\t\t\tBut I am really starting to see that while learning is\n\t\t\t\t\t\tincredibly important, school itself really isn't. Well,\n\t\t\t\t\t\tlet me rephrase that. Certain very time-consuming\n\t\t\t\t\t\taspects of school, like grades, really aren''t that important.\n\t\t\t\t\t\tMy time would be much better spent ensuring that I have\n\t\t\t\t\t\tlearned the material, and not worrying that much about\n\t\t\t\t\t\tgrades at all. All the time I can save here can be reinvested\n\t\t\t\t\t\tin the people around me. And even if I am not\n\t\t\t\t\t\tsuccessful in truly convincing myself to put less time into\n\t\t\t\t\t\tmy schoolwork, there is still a whole lot of wasted time\n\t\t\t\t\t\tin my life (see 1), that I can get a hold of and redirect to\n\t\t\t\t\t\tthe people that I care about (and even finding new people\n\t\t\t\t\t\tto care about!).\n\t\t\t\t\t\t</li>\n\t\t\t\t\t\t<li><strong>Change the way I eat food.</strong> No, I am not\n\t\t\t\t\t\tgoing on a diet. This resolution has nothing to do with what\n\t\t\t\t\t\tor how much I eat, but rather <i>why</i> I eat. I am very happy\n\t\t\t\t\t\twith my weight and overall think I have a pretty strong diet.\n\t\t\t\t\t\tBut I do, far too often, find myself eating for reasons other than\n\t\t\t\t\t\t\"I'm hungry.\" The most sinister of these is eating out of habit.\n\t\t\t\t\t\tIf I normally get a milkshake from Five Guys on Mondays, then\n\t\t\t\t\t\tI often find myself getting a milkshake from Five Guys on Mondays, even if I am not hungry in the least! These patterns\n\t\t\t\t\t\tare going to be hard to break, but ultimately it is very important\n\t\t\t\t\t\tthat I do it now, when my metabolism is still forgiving. There\n\t\t\t\t\t\tare other reasons for eating, like stress, boredom, or socializing\n\t\t\t\t\t\tthat I hope to break, but if I can eradicate habitual eating, I will\n\t\t\t\t\t\tconsider this resolution a success. </li>\n\t\t\t\t\t\t</ol>\n\n\t\t\t\t\t\t<p>So there they are. I intentionally left my resolutions a little bit vague, without too many specific ways to achieve them. This is because my life at the moment can be so sporadic that I\n\t\t\t\t\t\tthink getting to nitty-gritty with my resolutions will sharply\n\t\t\t\t\t\tdecrease my chance of attaining them. I intend to post\n\t\t\t\t\t\tupdates as the year progresses. Happy New Year!\n\t\t\t\t\t\t",
        "date": "12/26/2017",
        "location": "blog/2017/resolutions.html",
        "name": "2017 New Years Resolutions"
    },
    {
        "content": "\n                    <p>One of the principal components (pun intended) of my Master's\n                    program at UVA is the completion of a Capstone research project.\n                    Now that I am halfway through the program, I feel it is a good time\n                    to do an overview of the project, explain what work we have\n                    completed so far, and a few things I've picked up along the way.</p>\n\n                    <h3>Overview</h3>\n\n                    <p>\n                        Our project is titled, \"Using Speech to Predict Violence in Value-Based Groups,\" or something like that. Basically, humans like to collate around common ideas, or \"values\", in our case. Being that\n                        humans are intrinsically social creatures, this formation of groups\n                        is great!... most of the time. Sometimes, members of these groups,\n                        inspired by ideas within the group, decide to commit acts of violence. Given that new groups like this are popping up all the time in less understood areas of the globe, it would be life-saving to figure out what groups are going\n                        to be violent before we (our military) has to interact with them. That\n                        is the crux of our research.\n                    </p>\n\n                    <p>\n                        Our capstone is actually the continuation of two years of prior\n                        capstone projects. These capstone project aimed to predict a metric\n                        assigned to groups called \"linguistic rigidity.\" Linguistic rigidity\n                        is given to texts from a given group and aims to quantify the, well,\n                        rigidity of the language a group uses. It's a bit unintuitive but the\n                        idea is that this \"linguistic rigidity\" is a proxy for the violent potential of a group. These groups showed promising results but\n                        we felt that their analyses had a couple of areas for improvement. The first being\n                        that linguistic rigidity seemed like an unnecessary middleman for\n                        the thing we actually care about, which is predicting group violence.\n                        Second, the analysis was restricted to English. The researchers used\n                        their knowledge of English to help inform the models what to look\n                        for. This is problematic since many of the groups appear in parts of\n                        the globe that do not primarily speak English, thus their analysis is\n                        largely void for the majority of the world.\n                    </p>\n\n                    <p>\n                        Our capstone hopes to address both of these issues. Instead of trying to predict linguistic rigidity, we are going straight to trying to predict\n                        violence. Further, we are conducting our experiment in a \"language\n                        agnostic\" manner. What this means is that nothing in our analysis is\n                        specific to the language being used. Theoretically, you should be able\n                        to plug in any language into our pipeline and get comparable results.\n                        As a proof of concept, we are conducting our analysis in Arabic,\n                        which none of us speak (well, one of our advisors does, but what are\n                        you gonna do?).\n                    </p>\n\n                    <h3>Definitions, Definitions, Definitions</h3>\n\n                    <p>Before we can get into the collections or our data, we need\n                        to establish some definitions. This has been a first for me since most\n                        of the projects I do are pretty straightforward and objective. But\n                        since this project falls more into the domain of human behavior,\n                        we need to establish some definitions. A fair question would be\n                        how we decided on definitions. The short answer is, we research\n                        how other people defined them and tweak the definitions to be\n                        most sensible for our use case. A fair followup would be, \"but\n                        doesn't that introduce some experimenter bias?\" Yes, but anyway you define subjective term will introduce bias, so let's accept that and move on.</p>\n\n                    <p>We know we need to find text's from violent and non-violent value\n                        based groups. But what the hell do most of the words in that last sentence mean in a practical sense? In the interest of not dragging on, I'm just going to list the answers to a bunch of preliminary questions we faced.</p>\n                    <ul style=\"padding: 0em 5em\">\n                        <li>\n                            <i>What is violence?</i> We ended up basing our definition\n                            of violence on a few different definitions we found. We define\n                            violence to be any intentional act that has a high probability of causing injury or death to humans or animals.\n                        </li>\n                        <li>\n                            <i>What is a violent group?</i> A violent group is any group who has\n                            members that commit an act of violence as described above and the group takes responsibility for the action.\n                        </li>\n                        <li>\n                            <i>What is a value?</i> Pretty much anything that somebody thinks.\n                        </li>\n                        <li>\n                            <i>What is a value based group?</i> Any group that operates under\n                            a common name and has either an explicit publically available set\n                            of motives/values or whose values are obvious from their behavior.\n                            The group must also have a primary motive outside of making a profit.\n                        </li>\n                        <li>\n                            <i>What is a document?</i> Any media that a group or members of a group put out that can be extracted into a text document. This includes web pages, PDFs, speeches (if we ever figure out a scalable way to do speech to text).</li>\n                        <li>\n                            <i>How are documents labeled?</i> All documents within a group are given the same label. So if ISIS is labeled as violent, all documents from ISIS are considered violent documents\n                        </li>\n                    <li>\n                        <i>Is that realistic?</i> Probably not, but it's a start. Future work, either by us or other researchers, should focus on creating a more\n                        granular definition of violence that can allow documents to be\n                        judged at a more individual level. This will introduce far less bias\n                        as groups change from non-violent to violent, or vice-versa, pretty frequently.\n                    </li>\n                    <li>\n                        <i>What does it mean for an analysis to be language agnostic?</i> The analysis should not do anything that is specific to a language\n                        and cannot be extended to another language. For example, part of speech tagging tools are not language agnostic as they do not\n                        exist readily for all languages. However, stop-word analysis is valid since this really just requires finding what words are used\n                        extremely often, which can be done with any language.\n                    </li>\n                    <li>\n                        <i>Can a person be a group?</i> Yes, after much debate we decided\n                        that an individual person can be a 'value-based group' if they are\n                        expressing values under their name, like religious scholars.\n                    </li>\n                    <li>\n                        <i>How about news organizations?</i> We ended up also letting news\n                        organizations be value-based groups, but only the opinion and op-ed\n                        sections where people are expressing their values.\n                    </li>\n                    </ul>\n\n            <h3>Getting the Data Otherwise Known as our Entire Semester</h3>\n\n            <p>\n                You know that mantra that 90% of a data scientist's work is data\n                wrangling/preprocessing? Well, I'd like to submit my Fall 2017 into\n                evidence. I wrote more web scrappers this past semester than I\n                thought I would in my entire life, but we'll get more into that in\n                a bit. First, let's talk about how we even found data. Actually, we need to go back even further. How do we even decide what data we needed? Luckily for me, most of the work answering these questions\n                fell to one of the other members of my team. This member identified\n                various Arabic speaking groups throughout the globe who seemed\n                to be large and influential enough to have some sort of website\n                findable by Google. After identifying these groups, he scoured the\n                web until he was able to find either official sites for these groups,\n                or just sites that hosted their content. For example, he found a\n                site that took newsletters ISIS released and upload those. In\n                one case, another member of our groups found a University that\n                had scrapped the dark web for a number of years and hosted a\n                number of those results for free. We found one forum that fit out\n                criteria and were able to use it.</p>\n\n            <p>Then came the web scrapers. Ohhhhh the web scrapers. Given\n                that I had the most background with programming, I was tasked\n                with scraping all of the websites. I was given a list of URLs, and\n                that's it. From there, I had free reign. Now, in order to scrape all\n                of the blogs and forums in a scalable way, I researched for some\n                pre-built scraping tools or libraries I could utilize. Now, don't get me\n                wrong, there are a lot of mature and thorough tools for scraping\n                websites, Scrapy being an especially good one, but they all ended\n                up being useless for me. The reason for this is simple: they require\n                the sites they scrape to be somewhat well written and predictable.\n                Turns out, terrorists write shitty websites. Who knew?! I don't know\n                if it's lack of technical prowess or just designing poorly fits in their\n                evil brand, but the result is the same.  A series of poorly written sites\n                that require hacked together, custom web scrappers to scrape. Ugh.</p>\n\n            <p>I ended up going with the tried and true: requests and BeautifulSoup (and in one case, a library to bypass CloudFlare\n                because terrorists really need DDoS protection). Basically, I\n                had to look for patterns in the URL, or for embedded URLs in\n                a webpage, and also some good markers to extract out the\n                relevant text from the potpourri of text that BeautifulSoup\n                pulls out. All in all, it was just a huge, tedious, pain in the\n                butt. I spend about 10 hours over the course of a weekend,\n                hold up in a library, doing nothing but writing and running\n                web scrapers. </p>\n\n            <p>I'm going to stop here for now. But please read on to part 2 to\n                find out more, including how we preprocessed the data, feature engineering (or\n                lack thereof), and some preliminary results.</p>\n            ",
        "date": "2/6/2018",
        "location": "blog/2018/capstone1.html",
        "name": "Capstone Semester One Wrap Up (Part 1)"
    },
    {
        "content": "\n\n                    <p>Welcome back to the second part of the wrap up of the capstone work completed during my first semester\n                        of my Master's in Data Science Program. At this point, we have defined the problem and collected the data,\n                        so now it's time for the fun stuff.\n                    </p>\n\n                    <h3>To Preprocess, or not to Preprocess</h3>\n\n                    <p>In some ways, that isn't even the right question to ask. Of course\n                    we are going to have to preprocess, but how much and in what ways\n                    are key for us. We are touting these models as \"language agnostic,\"\n                    and the agnosticism must start at the preprocessing stage. So at\n                    this stage in our project, we have about 70,000 different documents,\n                    representing 20 different groups with an even split of violent and\n                    non-violent groups and at the end of the preprocessing pipeline, we\n                    should have a \"clean\" set of document that are ready to be feature\n                    engineered.</p>\n\n                    <p>Step one is quite clear: remove the junk. Junk is junk regardless\n                    of what language it is in so we can safely remove it. By junk, we\n                    mean anything that is not an Arabic character or numeric character.\n                    This was quite cleanly achieved through the use of regular expressions. There also becomes the question of how to deal\n                    with numbers. Numbers obviously signal something in a document,\n                    but exactly what is often unclear. And when all number are included,\n                    the feature space literally becomes infinite (in theory), but even in\n                    practice becomes quite large. Instead, we opt to give numbers special\n                    characters, namely, NUM. This really does finish out the preprocessing of individual documents.</p>\n\n                    <p>The next step is to determine what documents belong in a ready\n                    to go corpora for this research. Short documents (i.e, a couple of words), add computation time without significantly adding to the\n                    corpora since it is hard to express intent in only a couple of words.\n                    After looking at the word distribution of all documents, we find that\n                    the median number of words in a document is about 200, with the\n                    mean tending towards 100. The min is 0 words and the max is over\n                    50,000. After a fair bit of deliberation, we somewhat arbitrarily decided at a cutoff of 90 words for a document to be included in the\n                    corpus. After removing the runts of the group, we are left with about\n                    55,000 documents, more than enough to get us started.</p>\n\n                    <h3>The Features are Coming</h3>\n\n                    <p>Obviously, raw text is meaningless to any sort of machine learning model. One of the principal natural language processing (NLP) problems is how to convert\n                    from raw text to numbers that we can feed into our models. There\n                    have been great strides forward in NLP in areas such as sentiment\n                    analysis (how is the author feeling) and part-of-speech tagging\n                    (identifying the part of speech for any given word). Unfortunately,\n                    we cannot use any of that. Knowing enough about a language to\n                    identify author sentiment is far beyond that language-neutral approach that we seek. Thus, we have to go back the basics. The\n                    primary features extraction techniques that we use are TF-IDF\n                    weights and word embeddings.</p>\n\n                    <p>Without this post becoming an NLP lesson, I will touch on what\n                    these weights mean. TF-IDF (text frequency-inverse document frequency) weights map documents to feature vectors which attempt to quantify what words a document uses, how often it uses\n                    those words, and how effective those words are identifying the\n                    document. This last caveat ensures that common words, like \"the\" in\n                    English, aren't given high weights because even though they appear very often within any given document, they appear with the similar frequencies in almost every document, so they are ineffective at\n                    identifying any one document. Word embeddings are a very different\n                    approach. Instead of mapping documents to features vectors, they map words to feature vectors where the feature vector for any given\n                    word attempts to quantify the context with which that word appears. <i><strong>Caution, technical terms ahead.</strong></i> For\n                    the more technical reader who desires specifics, we actually used\n                    sublinear TF-IDF weights and the word2vec implementation of word embeddings. We trained our own word embeddings as opposed to\n                    using pre-trained since our use case is quite specific. The TF-IDF weights were calculated using Java and the word embeddings are a feature of the Keras deep learning library in Python. For the TF-IDF weights, we chose the top 10,000 features according to the information gain of the features.</p>\n\n                    <h3>Models on Models on Models</h3>\n\n                    <p>After months of work, we finally get to the easy part, taking all\n                    of the data and plugging it into our models to finally get some results! Because this is research and we don't know what models\n                    will perform best, we decided to use all of them. While I am obviously kidding, we did cast our net pretty wide in terms\n                    of models. We consider a number of baseline models like logistic\n                    regression, random forests, support vector machines, and gradient\n                    boosted trees. These models all take in the TF-IDF weights as features. Our \"novel\" models are deep learning models such as\n                    convolutional neural networks and LSTM networks. Even though these are relatively mature infrastructures, we consider them \"novel\" because a) they haven't been around nearly as long as the\n                    baselines and b) we expect them to outperform the baselines.</p>\n\n                    <p>The logistic regression was run using custom Java code. After\n                    this, we hit a bit of a problem. All of the other baseline models are\n                    very easily implemented in Python using the sklearn library, but the\n                    TF-IDF weights are calculated and exist in Java. We need to find a way to export these feature vectors and import them to Python.\n                    Now, these TF-IDF weights exist in a matrix (well, not exactly. They're stored as dictionaries, but close enough), so exporting them as a csv should be trivial. However, the space of it all coming into play. Each document is at least a 10,000 x 200 matrix containing double values and there are about 55,000 documents. You're looking\n                    at quite a few gigs of memory to store matrices that mostly contain\n                    0's (TF-IDF matrices are usually extremely sparse). Instead, we used\n                    a sparse matrix representation for the matrices. Specifically, each matrix was represented as a giant list of coordinates where each coordinate specified what value should exist at that coordinate.\n                    From here, we could have converted to binary files to save a little more space, but even without that, the exported feature files were\n                    only about a gigabyte total. Reading in these coordinate-value files\n                    is pretty simple using scipy's implementation of sparse matrices.</p>\n\n                    <p>With our features now over in Python, we can just plug and play\n                    the rest of our baseline models. For each, we did a grid search to help\n                    narrow down on some reasonable values for the hyperparameters. As for the neural networks, we used Keras with a Tensorflow backend\n                    to implement the networks. Regrettably, I have not had a huge part in the neural networks (at least not yet), as the original architectures\n                    were based on code from a past project and thus did not need a whole lot. We have a couple more architectures in mind that I would\n                    have the chance to be more involved in.</p>\n\n                    <h3>\"Houston, we have a (bias) problem\"</h3>\n\n                    <p>We are now almost in a position to talk about some preliminary results. We first need to briefly discuss our validation metrics. We\n                    had two different validation techniques. The first was a standard\n                    cross-validation technique and the second was a leave-one-group-out\n                    cross-validation (LOGO-CV) which involves leaving all documents from a single group out from training and use those documents\n                    for testing. We care more about the LOGO-CV results since they\n                    are more real to the application of this capstone. In terms of metrics,\n                    we looked at both accuracy and the F-1 score for both violent and non-violent groups. </p>\n\n                    <p> Now for the results. I won't go into exact values here since they will be listed out in the\n                    paper. The initial results were roughly as expected. The results of the\n                    standard cross-validation were ridiculously high, like almost 100%. Upon further investigation, we found out that this was because the\n                    model was actually picking up on a group's name in the learning phase so when it saw that group in test, it already knew the classification. The LOGO-CV results saw a pretty huge drop-off.\n                    For some groups, it was getting close to 100% still, and for others, it\n                    completely misclassified every document. All of the groups that it was getting 0% on were non-violent groups. What this means was that the models were essentially almost always guessing \"violent\".\n                    There are some ideas we have for mitigation of that will be included in the next section.</p>\n\n                    <p>These results were roughly consistent\n                    across models. Of the baseline models, logistic regression did the overall best. Overall, the two neural network models did the best,\n                    but the logistic regression was relatively competitive. These results\n                    are to be expected. Since neural networks are good at self-generating\n                    features, we expect them to perform well in situations where we\n                    only provide generic features. The logistic regression also isn't all\n                    that much of a surprise since logistic regression seems to just do\n                    at least ok at most tasks.</p>\n\n                    <h3>So, what's next?</h3>\n\n                    <p>That's approximately what we did during the first semester.\n                    so where do we go from here? The first thing that needs to be\n                    addressed, and something that we already have started to address\n                    as I write this is the imbalance in predictive accuracy between violent and non-violent groups. One of the root causes of this initially\n                    was one of our groups, Al-Boraq. This was the group whose documents we got from a research group that had collected forum\n                    posts for Al-Boraq. What this meant is that we have ~55,000 documents for Al-Boraq and ~15,000 documents for every other group. This is a very lopsided distribution that needs to be addressed.\n                    So the first step will be to downsample Al-Boraq randomly.</p>\n\n                    <p>Another step will be to do random oversampling in order to obtain perfectly even class distributions (i.e, the exact same number\n                    of violent and non-violent documents). Implementation-wise, this is a\n                    very simple change and will likely have a sizable effect. Another potential, but less feasible option, would be to diversify the corpus by\n                    getting documents from even more groups. Given the time-frame, this is unlikely to happen.</p>\n\n                    <p>We also may have to reconcile the fact that there is a fair bit of irreducible error in our data, where some documents are almost\n                    carbon copies of others but are labeled as both violent and non-violent. There are two obvious examples of this. Violent religious groups will often quote non-violent religious scholars in order\n                    to legitimize their cause. On the other hand, non-violent news\n                    organizations may post direct quotes from violent groups when\n                    reporting on them. It's hard to say how frequent these cases are,\n                    but they are almost impossible to rectify.</p>\n\n                    <p>Other than solving this issue, we have a few other ideas in mind\n                    for this semester. The first is to implement a few other neural network architectures, mainly GRUs and RCNN (recurrent convolutional neural networks) to see if they improve on the\n                    results of the other neural networks. Another really cool, but\n                    likely overly ambitious, idea would be to reverse engineer the\n                    CNN. This idea has been implemented with images, where you\n                    can see what inputs cause certain output neurons to fire by\n                    tracing the network backward. To our knowledge, this has not\n                    been done with text, so it may be too arduous.</p>\n\n                    <h3>Cool, but what have I learned?</h3>\n\n                    <p>After a grueling two blogs posts, we are almost there.\n                    But I would be remiss if I didn't go through a couple of things\n                    that I have learned so far. I'm more exausted writing this than you\n                    are reading, so I'm lazily just going to put these lessons in list form.\n                    <ul style=\"padding: 0em 5em\">\n                    <li>\n                    Research is a whole different beast from anything else that I have done, for better or (more often than not) worse. Doing any sort of\n                    research project comes with a couple of intrinsic evils that I have yet\n                    to come across in my academic career. The first is, as opposed to the nice, structured problems that you come across in classes, your research just might not work. You could spend weeks working on some part of your project to have it yield no benefit whatsoever.\n                    This is certainly something I will need to get used to because such\n                    things happen in industry all of the time, but it certainly is frustrating the first time you come across it. Also, the lack of\n                    specifications can be rather tumultuous. If you are working on a\n                    project for an end user, you can narrow in your specifications by\n                    prototyping and then interviewing the end user. In research, the end user\n                    is.........well, nobody. We have a sponsor but the overall end goal is to expand human knowledge in a certain domain. But how that is done\n                    is open-ended. The specifications are chosen by you and are altered\n                    based on what works and what doesn't. The ambiguity can be quite\n                    burdensome in terms of scheduling how long any given part of the\n                    project will take. If you can't tell, I think doing research has been an\n                    invaluable experience, but I don't think it's for me.\n                    </li>\n                    <li>\n                    Auditing is super important, especially as the project gets more complicated. Auditing is kind of the same as test cases in coding,\n                    but they are distinct because they can't really be automated or\n                    even anticipated. In coding, the testing pipeline is \"this is what\n                    my app needs to do, I can write some tests ahead of time to validate\n                    those features and then develop until the product can pass all of the\n                    tests.\" Such a pipeline does not translate to large research projects. You can, and should, write test cases for your code, but research, and\n                    data science in general, needs something else. Many times, you will perform some sort of analysis and get results, and the auditing is\n                    going in and making sure that your analytical procedure is valid\n                    and that your results are reasonable. This can be a tedious process\n                    because it should really be done after any bit of analysis, but can't\n                    really be automated. You just have to do it.\n                    </li>\n                    </ul>\n\n                    <p>That's it for now. I will make another post later on in the semester wrapping up\n                    the project, as well as linking our published paper.<p>\n            ",
        "date": "2/6/2018",
        "location": "blog/2018/capstone1.html",
        "name": "Capstone Semester One Wrap Up (Part 2)"
    }
]