[
    {
        "content": "\n\t\t\t\t\t\t\t<p>Here we are, my very first blog post. The main objective of\n\t\t\t\t\t\t\tthis post is to give a little taste of who I am as a person\n\t\t\t\t\t\t\tand also motivate the existence of this site.</p>\n\n\t\t\t\t\t\t\t<p>So who am I? Well, at risk of plagiarising my own Linked-In profile, I am a graduate student at the University of Virginia\n\t\t\t\t\t\t\tcurrently pursuing my Masters in Data Science. I chose to pursue this degree as I saw Data Science as the most cutting-edge manifestation of the subjects I studied as an undergrad\n\t\t\t\t\t\t\t(I double majored in Computer Science and Mathematics with a concentration in Probability and Statistics, also at the University of Virginia). I will complete\n\t\t\t\t\t\t\tthis degree in May 2018, so I am also going through the trials\n\t\t\t\t\t\t\tand tribulations of a first-time job seeker. That being said, I do\n\t\t\t\t\t\t\tnot want to get into too much more detail about myself here,\n\t\t\t\t\t\t\tbecause that is what this site is about! Exploring it should give\n\t\t\t\t\t\t\tyou a relatively complete picture of who I am and what I am\n\t\t\t\t\t\t\tall about.</p>\n\n\t\t\t\t\t\t\t<p>So why exactly am I making a site for myself? I've listed\n\t\t\t\t\t\t\ta few compelling reasons on the homepage of this site,\n\t\t\t\t\t\t\tbut really the motivation for this site came from enough\n\t\t\t\t\t\t\tpeople telling me that it would be a good idea to have\n\t\t\t\t\t\t\ta personal site. I put off making one for various reasons,\n\t\t\t\t\t\t\tbut once I discovered that I already used (Github), had\n\t\t\t\t\t\t\ta mechanism for hosting static websites for free (Github Pages),\n\t\t\t\t\t\t\tI was officially out of excuses. I can think of a few additional reasons beyond what has already been listed. I think the exercise of publishing content could very well change the way\n\t\t\t\t\t\t\tI interact with the world. Whenever I am undergoing some\n\t\t\t\t\t\t\tinteresting or challenging problem in my life, I can view it through the lens of something that can be written up and\n\t\t\t\t\t\t\tposted to the world. I imagine that this will\n\t\t\t\t\t\t\thelp me engage with the world around me. Publishing is also\n\t\t\t\t\t\t\ta good mechanism to help hold myself accountable. My next\n\t\t\t\t\t\t\tblog post is going to involve my 2017-2018 New Years resolutions and the hope is that putting them out in a more\n\t\t\t\t\t\t\tpermanent way online will help me to actually remember and\n\t\t\t\t\t\t\tfollow through on them.</p>\n\n\t\t\t\t\t\t\t<p>So what exactly do I want this site to be? At the end of the day,\n\t\t\t\t\t\t\tI just it to be the most complete digital picture of myself out\n\t\t\t\t\t\t\tthere, and one that I have complete and total control over.\n\t\t\t\t\t\t\tBeyond that, I do not know what this site could become. Most\n\t\t\t\t\t\t\tnotably, I do not want to set any sort of goal or theme for this\n\t\t\t\t\t\t\tblog. It will certainly contain updates on my Capstone research\n\t\t\t\t\t\t\tand experience as a first-time job hunter, but I do not want to\n\t\t\t\t\t\t\tlimit myself to those things. This is really an outlet to share\n\t\t\t\t\t\t\tanything in my life that I think others might find useful.</p>\n\n\t\t\t\t\t\t\t<p>Will that in mind, I hope that you will explore the rest of my site and I can't wait to see how this project evolves!</p>\n\t\t\t\t\t\t\t",
        "date": "12/23/2017",
        "location": "blog/2017/introduction.html",
        "name": "Introduction Blog"
    },
    {
        "content": "\n\t\t\t\t\t\t<p>I'm going to be honest here, I am not the biggest fan of New\n\t\t\t\t\t\tYears Resolutions. Don't get me wrong, I think the idea of\n\t\t\t\t\t\tidentifying areas for self-improvement and working\n\t\t\t\t\t\ttowards those goals is great, I just think waiting for\n\t\t\t\t\t\ta calendar to tell you \"it's time to improve yourself\"\n\t\t\t\t\t\tis a bit silly. However, I do think the New Years at least\n\t\t\t\t\t\tprovides a time to sit back and reflect, especially since\n\t\t\t\t\t\tit is around this time that people, myself included, have\n\t\t\t\t\t\tsome time off and therefore can spend time reflecting.\n\t\t\t\t\t\tI've decided this year to publish my New Years Resolutions,\n\t\t\t\t\t\tmostly so I have a tangible and public set of goals, so\n\t\t\t\t\t\tanyone that reads this and knows me can hold me accountable to these resolutions. Also, I frankly do not remember what\n\t\t\t\t\t\tmy resolutions last year were, so having them easily accessible\n\t\t\t\t\t\tshould increase my chances of actually doing them.</p>\n\n\t\t\t\t\t\t<p>Without further ado, let's jump into my New Years Resolutions for 2018.<p>\n\t\t\t\t\t\t<ol>\n\t\t\t\t\t\t<li> <strong>Consume less, create more.</strong> I do not\n\t\t\t\t\t\tconsider myself a procrastinator. In fact, I am pretty good\n\t\t\t\t\t\tat getting things done well before they need to be done.\n\t\t\t\t\t\tHowever, I am quite adept at wasting time. Too much of\n\t\t\t\t\t\tmy day is spent consuming content that adds no real\n\t\t\t\t\t\tvalue to my life. Youtube and Netflix are the worst offenders here, with\n\t\t\t\t\t\tFacebook and Snapchat being close runner-ups. A simple\n\t\t\t\t\t\tway to achieve this goal would be to remove myself from\n\t\t\t\t\t\tthose platforms, but I do not see that as realistic. There are\n\t\t\t\t\t\tvery good things those platforms have to offer, I just need\n\t\t\t\t\t\tto be better at practicing moderation. More tangibly,\n\t\t\t\t\t\tI need to not use them just as a means to kill time. What\n\t\t\t\t\t\tdo I plan on doing with all the time I'll be getting back?\n\t\t\t\t\t\tWell, create more, whatever that may look like. This\n\t\t\t\t\t\twebsite is a great start, so maintaining and growing it\n\t\t\t\t\t\tout will be a good way to work towards this goal. Getting back\n\t\t\t\t\t\tinto writing music, and maybe this time actually record some\n\t\t\t\t\t\tof it, will also be valuable in achieving this goal.\n\t\t\t\t\t\tContinuing to work on the <i>projmanr</i> R package and\n\t\t\t\t\t\t<i>Super Smash Bros. SE</i> game mod will also be productive as well.</li>\n\n\t\t\t\t\t\t<li><strong>Invest more in people.</strong> I admit it,\n\t\t\t\t\t\tI can be a bit of a workaholic. For easily the past decade\n\t\t\t\t\t\tof my life, school has been the most important aspect\n\t\t\t\t\t\tof my life, especially my last four years in college.\n\t\t\t\t\t\tThis emphasis on my education has really led me\n\t\t\t\t\t\tto neglect the people in my life, especially my group\n\t\t\t\t\t\tof friends.\n\t\t\t\t\t\tBut I am really starting to see that while learning is\n\t\t\t\t\t\tincredibly important, school itself really isn't. Well,\n\t\t\t\t\t\tlet me rephrase that. Certain very time-consuming\n\t\t\t\t\t\taspects of school, like grades, really aren''t that important.\n\t\t\t\t\t\tMy time would be much better spent ensuring that I have\n\t\t\t\t\t\tlearned the material, and not worrying that much about\n\t\t\t\t\t\tgrades at all. All the time I can save here can be reinvested\n\t\t\t\t\t\tin the people around me. And even if I am not\n\t\t\t\t\t\tsuccessful in truly convincing myself to put less time into\n\t\t\t\t\t\tmy schoolwork, there is still a whole lot of wasted time\n\t\t\t\t\t\tin my life (see 1), that I can get a hold of and redirect to\n\t\t\t\t\t\tthe people that I care about (and even finding new people\n\t\t\t\t\t\tto care about!).\n\t\t\t\t\t\t</li>\n\t\t\t\t\t\t<li><strong>Change the way I eat food.</strong> No, I am not\n\t\t\t\t\t\tgoing on a diet. This resolution has nothing to do with what\n\t\t\t\t\t\tor how much I eat, but rather <i>why</i> I eat. I am very happy\n\t\t\t\t\t\twith my weight and overall think I have a pretty strong diet.\n\t\t\t\t\t\tBut I do, far too often, find myself eating for reasons other than\n\t\t\t\t\t\t\"I'm hungry.\" The most sinister of these is eating out of habit.\n\t\t\t\t\t\tIf I normally get a milkshake from Five Guys on Mondays, then\n\t\t\t\t\t\tI often find myself getting a milkshake from Five Guys on Mondays, even if I am not hungry in the least! These patterns\n\t\t\t\t\t\tare going to be hard to break, but ultimately it is very important\n\t\t\t\t\t\tthat I do it now, when my metabolism is still forgiving. There\n\t\t\t\t\t\tare other reasons for eating, like stress, boredom, or socializing\n\t\t\t\t\t\tthat I hope to break, but if I can eradicate habitual eating, I will\n\t\t\t\t\t\tconsider this resolution a success. </li>\n\t\t\t\t\t\t</ol>\n\n\t\t\t\t\t\t<p>So there they are. I intentionally left my resolutions a little bit vague, without too many specific ways to achieve them. This is because my life at the moment can be so sporadic that I\n\t\t\t\t\t\tthink getting to nitty-gritty with my resolutions will sharply\n\t\t\t\t\t\tdecrease my chance of attaining them. I intend to post\n\t\t\t\t\t\tupdates as the year progresses. Happy New Year!\n\t\t\t\t\t\t",
        "date": "12/26/2017",
        "location": "blog/2017/resolutions.html",
        "name": "2017 New Years Resolutions"
    },
    {
        "content": "\n                    <p>One of the principal components (pun intended) of my Master's\n                    program at UVA is the completion of a Capstone research project.\n                    Now that I am halfway through the program, I feel it is a good time\n                    to do an overview of the project, explain what work we have\n                    completed so far, and a few things I've picked up along the way.</p>\n\n                    <h3>Overview</h3>\n\n                    <p>\n                        Our project is titled, \"Using Speech to Predict Violence in Value-Based Groups,\" or something like that. Basically, humans like to collate around common ideas, or \"values\", in our case. Being that\n                        humans are intrinsically social creatures, this formation of groups\n                        is great!... most of the time. Sometimes, members of these groups,\n                        inspired by ideas within the group, decide to commit acts of violence. Given that new groups like this are popping up all the time in less understood areas of the globe, it would be life-saving to figure out what groups are going\n                        to be violent before we (our military) has to interact with them. That\n                        is the crux of our research.\n                    </p>\n\n                    <p>\n                        Our capstone is actually the continuation of two years of prior\n                        capstone projects. These capstone project aimed to predict a metric\n                        assigned to groups called \"linguistic rigidity.\" Linguistic rigidity\n                        is given to texts from a given group and aims to quantify the, well,\n                        rigidity of the language a group uses. It's a bit unintuitive but the\n                        idea is that this \"linguistic rigidity\" is a proxy for the violent potential of a group. These groups showed promising results but\n                        we felt that their analyses had a couple of areas for improvement. The first being\n                        that linguistic rigidity seemed like an unnecessary middleman for\n                        the thing we actually care about, which is predicting group violence.\n                        Second, the analysis was restricted to English. The researchers used\n                        their knowledge of English to help inform the models what to look\n                        for. This is problematic since many of the groups appear in parts of\n                        the globe that do not primarily speak English, thus their analysis is\n                        largely void for the majority of the world.\n                    </p>\n\n                    <p>\n                        Our capstone hopes to address both of these issues. Instead of trying to predict linguistic rigidity, we are going straight to trying to predict\n                        violence. Further, we are conducting our experiment in a \"language\n                        agnostic\" manner. What this means is that nothing in our analysis is\n                        specific to the language being used. Theoretically, you should be able\n                        to plug in any language into our pipeline and get comparable results.\n                        As a proof of concept, we are conducting our analysis in Arabic,\n                        which none of us speak (well, one of our advisors does, but what are\n                        you gonna do?).\n                    </p>\n\n                    <h3>Definitions, Definitions, Definitions</h3>\n\n                    <p>Before we can get into the collections or our data, we need\n                        to establish some definitions. This has been a first for me since most\n                        of the projects I do are pretty straightforward and objective. But\n                        since this project falls more into the domain of human behavior,\n                        we need to establish some definitions. A fair question would be\n                        how we decided on definitions. The short answer is, we research\n                        how other people defined them and tweak the definitions to be\n                        most sensible for our use case. A fair followup would be, \"but\n                        doesn't that introduce some experimenter bias?\" Yes, but anyway you define subjective term will introduce bias, so let's accept that and move on.</p>\n\n                    <p>We know we need to find text's from violent and non-violent value\n                        based groups. But what the hell do most of the words in that last sentence mean in a practical sense? In the interest of not dragging on, I'm just going to list the answers to a bunch of preliminary questions we faced.</p>\n                    <ul style=\"padding: 0em 5em\">\n                        <li>\n                            <i>What is violence?</i> We ended up basing our definition\n                            of violence on a few different definitions we found. We define\n                            violence to be any intentional act that has a high probability of causing injury or death to humans or animals.\n                        </li>\n                        <li>\n                            <i>What is a violent group?</i> A violent group is any group who has\n                            members that commit an act of violence as described above and the group takes responsibility for the action.\n                        </li>\n                        <li>\n                            <i>What is a value?</i> Pretty much anything that somebody thinks.\n                        </li>\n                        <li>\n                            <i>What is a value based group?</i> Any group that operates under\n                            a common name and has either an explicit publically available set\n                            of motives/values or whose values are obvious from their behavior.\n                            The group must also have a primary motive outside of making a profit.\n                        </li>\n                        <li>\n                            <i>What is a document?</i> Any media that a group or members of a group put out that can be extracted into a text document. This includes web pages, PDFs, speeches (if we ever figure out a scalable way to do speech to text).</li>\n                        <li>\n                            <i>How are documents labeled?</i> All documents within a group are given the same label. So if ISIS is labeled as violent, all documents from ISIS are considered violent documents\n                        </li>\n                    <li>\n                        <i>Is that realistic?</i> Probably not, but it's a start. Future work, either by us or other researchers, should focus on creating a more\n                        granular definition of violence that can allow documents to be\n                        judged at a more individual level. This will introduce far less bias\n                        as groups change from non-violent to violent, or vice-versa, pretty frequently.\n                    </li>\n                    <li>\n                        <i>What does it mean for an analysis to be language agnostic?</i> The analysis should not do anything that is specific to a language\n                        and cannot be extended to another language. For example, part of speech tagging tools are not language agnostic as they do not\n                        exist readily for all languages. However, stop-word analysis is valid since this really just requires finding what words are used\n                        extremely often, which can be done with any language.\n                    </li>\n                    <li>\n                        <i>Can a person be a group?</i> Yes, after much debate we decided\n                        that an individual person can be a 'value-based group' if they are\n                        expressing values under their name, like religious scholars.\n                    </li>\n                    <li>\n                        <i>How about news organizations?</i> We ended up also letting news\n                        organizations be value-based groups, but only the opinion and op-ed\n                        sections where people are expressing their values.\n                    </li>\n                    </ul>\n\n            <h3>Getting the Data Otherwise Known as our Entire Semester</h3>\n\n            <p>\n                You know that mantra that 90% of a data scientist's work is data\n                wrangling/preprocessing? Well, I'd like to submit my Fall 2017 into\n                evidence. I wrote more web scrappers this past semester than I\n                thought I would in my entire life, but we'll get more into that in\n                a bit. First, let's talk about how we even found data. Actually, we need to go back even further. How do we even decide what data we needed? Luckily for me, most of the work answering these questions\n                fell to one of the other members of my team. This member identified\n                various Arabic speaking groups throughout the globe who seemed\n                to be large and influential enough to have some sort of website\n                findable by Google. After identifying these groups, he scoured the\n                web until he was able to find either official sites for these groups,\n                or just sites that hosted their content. For example, he found a\n                site that took newsletters ISIS released and upload those. In\n                one case, another member of our groups found a University that\n                had scrapped the dark web for a number of years and hosted a\n                number of those results for free. We found one forum that fit out\n                criteria and were able to use it.</p>\n\n            <p>Then came the web scrapers. Ohhhhh the web scrapers. Given\n                that I had the most background with programming, I was tasked\n                with scraping all of the websites. I was given a list of URLs, and\n                that's it. From there, I had free reign. Now, in order to scrape all\n                of the blogs and forums in a scalable way, I researched for some\n                pre-built scraping tools or libraries I could utilize. Now, don't get me\n                wrong, there are a lot of mature and thorough tools for scraping\n                websites, Scrapy being an especially good one, but they all ended\n                up being useless for me. The reason for this is simple: they require\n                the sites they scrape to be somewhat well written and predictable.\n                Turns out, terrorists write shitty websites. Who knew?! I don't know\n                if it's lack of technical prowess or just designing poorly fits in their\n                evil brand, but the result is the same.  A series of poorly written sites\n                that require hacked together, custom web scrappers to scrape. Ugh.</p>\n\n            <p>I ended up going with the tried and true: requests and BeautifulSoup (and in one case, a library to bypass CloudFlare\n                because terrorists really need DDoS protection). Basically, I\n                had to look for patterns in the URL, or for embedded URLs in\n                a webpage, and also some good markers to extract out the\n                relevant text from the potpourri of text that BeautifulSoup\n                pulls out. All in all, it was just a huge, tedious, pain in the\n                butt. I spend about 10 hours over the course of a weekend,\n                hold up in a library, doing nothing but writing and running\n                web scrapers. </p>\n\n            <p>I'm going to stop here for now. But please read on to part 2 to\n                find out more, including how we preprocessed the data, feature engineering (or\n                lack thereof), and some preliminary results.</p>\n            ",
        "date": "2/6/2018",
        "location": "blog/2018/capstone1.html",
        "name": "Capstone Semester One Wrap Up (Part 1)"
    },
    {
        "content": "\n\n                    <p>Welcome back to the second part of the wrap up of the capstone work completed during my first semester\n                        of my Master's in Data Science Program. At this point, we have defined the problem and collected the data,\n                        so now it's time for the fun stuff.\n                    </p>\n\n                    <h3>To Preprocess, or not to Preprocess</h3>\n\n                    <p>In some ways, that isn't even the right question to ask. Of course\n                    we are going to have to preprocess, but how much and in what ways\n                    are key for us. We are touting these models as \"language agnostic,\"\n                    and the agnosticism must start at the preprocessing stage. So at\n                    this stage in our project, we have about 70,000 different documents,\n                    representing 20 different groups with an even split of violent and\n                    non-violent groups and at the end of the preprocessing pipeline, we\n                    should have a \"clean\" set of document that are ready to be feature\n                    engineered.</p>\n\n                    <p>Step one is quite clear: remove the junk. Junk is junk regardless\n                    of what language it is in so we can safely remove it. By junk, we\n                    mean anything that is not an Arabic character or numeric character.\n                    This was quite cleanly achieved through the use of regular expressions. There also becomes the question of how to deal\n                    with numbers. Numbers obviously signal something in a document,\n                    but exactly what is often unclear. And when all number are included,\n                    the feature space literally becomes infinite (in theory), but even in\n                    practice becomes quite large. Instead, we opt to give numbers special\n                    characters, namely, NUM. This really does finish out the preprocessing of individual documents.</p>\n\n                    <p>The next step is to determine what documents belong in a ready\n                    to go corpora for this research. Short documents (i.e, a couple of words), add computation time without significantly adding to the\n                    corpora since it is hard to express intent in only a couple of words.\n                    After looking at the word distribution of all documents, we find that\n                    the median number of words in a document is about 200, with the\n                    mean tending towards 100. The min is 0 words and the max is over\n                    50,000. After a fair bit of deliberation, we somewhat arbitrarily decided at a cutoff of 90 words for a document to be included in the\n                    corpus. After removing the runts of the group, we are left with about\n                    55,000 documents, more than enough to get us started.</p>\n\n                    <h3>The Features are Coming</h3>\n\n                    <p>Obviously, raw text is meaningless to any sort of machine learning model. One of the principal natural language processing (NLP) problems is how to convert\n                    from raw text to numbers that we can feed into our models. There\n                    have been great strides forward in NLP in areas such as sentiment\n                    analysis (how is the author feeling) and part-of-speech tagging\n                    (identifying the part of speech for any given word). Unfortunately,\n                    we cannot use any of that. Knowing enough about a language to\n                    identify author sentiment is far beyond that language-neutral approach that we seek. Thus, we have to go back the basics. The\n                    primary features extraction techniques that we use are TF-IDF\n                    weights and word embeddings.</p>\n\n                    <p>Without this post becoming an NLP lesson, I will touch on what\n                    these weights mean. TF-IDF (text frequency-inverse document frequency) weights map documents to feature vectors which attempt to quantify what words a document uses, how often it uses\n                    those words, and how effective those words are identifying the\n                    document. This last caveat ensures that common words, like \"the\" in\n                    English, aren't given high weights because even though they appear very often within any given document, they appear with the similar frequencies in almost every document, so they are ineffective at\n                    identifying any one document. Word embeddings are a very different\n                    approach. Instead of mapping documents to features vectors, they map words to feature vectors where the feature vector for any given\n                    word attempts to quantify the context with which that word appears. <i><strong>Caution, technical terms ahead.</strong></i> For\n                    the more technical reader who desires specifics, we actually used\n                    sublinear TF-IDF weights and the word2vec implementation of word embeddings. We trained our own word embeddings as opposed to\n                    using pre-trained since our use case is quite specific. The TF-IDF weights were calculated using Java and the word embeddings are a feature of the Keras deep learning library in Python. For the TF-IDF weights, we chose the top 10,000 features according to the information gain of the features.</p>\n\n                    <h3>Models on Models on Models</h3>\n\n                    <p>After months of work, we finally get to the easy part, taking all\n                    of the data and plugging it into our models to finally get some results! Because this is research and we don't know what models\n                    will perform best, we decided to use all of them. While I am obviously kidding, we did cast our net pretty wide in terms\n                    of models. We consider a number of baseline models like logistic\n                    regression, random forests, support vector machines, and gradient\n                    boosted trees. These models all take in the TF-IDF weights as features. Our \"novel\" models are deep learning models such as\n                    convolutional neural networks and LSTM networks. Even though these are relatively mature infrastructures, we consider them \"novel\" because a) they haven't been around nearly as long as the\n                    baselines and b) we expect them to outperform the baselines.</p>\n\n                    <p>The logistic regression was run using custom Java code. After\n                    this, we hit a bit of a problem. All of the other baseline models are\n                    very easily implemented in Python using the sklearn library, but the\n                    TF-IDF weights are calculated and exist in Java. We need to find a way to export these feature vectors and import them to Python.\n                    Now, these TF-IDF weights exist in a matrix (well, not exactly. They're stored as dictionaries, but close enough), so exporting them as a csv should be trivial. However, the space of it all coming into play. Each document is at least a 10,000 x 200 matrix containing double values and there are about 55,000 documents. You're looking\n                    at quite a few gigs of memory to store matrices that mostly contain\n                    0's (TF-IDF matrices are usually extremely sparse). Instead, we used\n                    a sparse matrix representation for the matrices. Specifically, each matrix was represented as a giant list of coordinates where each coordinate specified what value should exist at that coordinate.\n                    From here, we could have converted to binary files to save a little more space, but even without that, the exported feature files were\n                    only about a gigabyte total. Reading in these coordinate-value files\n                    is pretty simple using scipy's implementation of sparse matrices.</p>\n\n                    <p>With our features now over in Python, we can just plug and play\n                    the rest of our baseline models. For each, we did a grid search to help\n                    narrow down on some reasonable values for the hyperparameters. As for the neural networks, we used Keras with a Tensorflow backend\n                    to implement the networks. Regrettably, I have not had a huge part in the neural networks (at least not yet), as the original architectures\n                    were based on code from a past project and thus did not need a whole lot. We have a couple more architectures in mind that I would\n                    have the chance to be more involved in.</p>\n\n                    <h3>\"Houston, we have a (bias) problem\"</h3>\n\n                    <p>We are now almost in a position to talk about some preliminary results. We first need to briefly discuss our validation metrics. We\n                    had two different validation techniques. The first was a standard\n                    cross-validation technique and the second was a leave-one-group-out\n                    cross-validation (LOGO-CV) which involves leaving all documents from a single group out from training and use those documents\n                    for testing. We care more about the LOGO-CV results since they\n                    are more real to the application of this capstone. In terms of metrics,\n                    we looked at both accuracy and the F-1 score for both violent and non-violent groups. </p>\n\n                    <p> Now for the results. I won't go into exact values here since they will be listed out in the\n                    paper. The initial results were roughly as expected. The results of the\n                    standard cross-validation were ridiculously high, like almost 100%. Upon further investigation, we found out that this was because the\n                    model was actually picking up on a group's name in the learning phase so when it saw that group in test, it already knew the classification. The LOGO-CV results saw a pretty huge drop-off.\n                    For some groups, it was getting close to 100% still, and for others, it\n                    completely misclassified every document. All of the groups that it was getting 0% on were non-violent groups. What this means was that the models were essentially almost always guessing \"violent\".\n                    There are some ideas we have for mitigation of that will be included in the next section.</p>\n\n                    <p>These results were roughly consistent\n                    across models. Of the baseline models, logistic regression did the overall best. Overall, the two neural network models did the best,\n                    but the logistic regression was relatively competitive. These results\n                    are to be expected. Since neural networks are good at self-generating\n                    features, we expect them to perform well in situations where we\n                    only provide generic features. The logistic regression also isn't all\n                    that much of a surprise since logistic regression seems to just do\n                    at least ok at most tasks.</p>\n\n                    <h3>So, what's next?</h3>\n\n                    <p>That's approximately what we did during the first semester.\n                    so where do we go from here? The first thing that needs to be\n                    addressed, and something that we already have started to address\n                    as I write this is the imbalance in predictive accuracy between violent and non-violent groups. One of the root causes of this initially\n                    was one of our groups, Al-Boraq. This was the group whose documents we got from a research group that had collected forum\n                    posts for Al-Boraq. What this meant is that we have ~55,000 documents for Al-Boraq and ~15,000 documents for every other group. This is a very lopsided distribution that needs to be addressed.\n                    So the first step will be to downsample Al-Boraq randomly.</p>\n\n                    <p>Another step will be to do random oversampling in order to obtain perfectly even class distributions (i.e, the exact same number\n                    of violent and non-violent documents). Implementation-wise, this is a\n                    very simple change and will likely have a sizable effect. Another potential, but less feasible option, would be to diversify the corpus by\n                    getting documents from even more groups. Given the time-frame, this is unlikely to happen.</p>\n\n                    <p>We also may have to reconcile the fact that there is a fair bit of irreducible error in our data, where some documents are almost\n                    carbon copies of others but are labeled as both violent and non-violent. There are two obvious examples of this. Violent religious groups will often quote non-violent religious scholars in order\n                    to legitimize their cause. On the other hand, non-violent news\n                    organizations may post direct quotes from violent groups when\n                    reporting on them. It's hard to say how frequent these cases are,\n                    but they are almost impossible to rectify.</p>\n\n                    <p>Other than solving this issue, we have a few other ideas in mind\n                    for this semester. The first is to implement a few other neural network architectures, mainly GRUs and RCNN (recurrent convolutional neural networks) to see if they improve on the\n                    results of the other neural networks. Another really cool, but\n                    likely overly ambitious, idea would be to reverse engineer the\n                    CNN. This idea has been implemented with images, where you\n                    can see what inputs cause certain output neurons to fire by\n                    tracing the network backward. To our knowledge, this has not\n                    been done with text, so it may be too arduous.</p>\n\n                    <h3>Cool, but what have I learned?</h3>\n\n                    <p>After a grueling two blogs posts, we are almost there.\n                    But I would be remiss if I didn't go through a couple of things\n                    that I have learned so far. I'm more exausted writing this than you\n                    are reading, so I'm lazily just going to put these lessons in list form.\n                    <ul style=\"padding: 0em 5em\">\n                    <li>\n                    Research is a whole different beast from anything else that I have done, for better or (more often than not) worse. Doing any sort of\n                    research project comes with a couple of intrinsic evils that I have yet\n                    to come across in my academic career. The first is, as opposed to the nice, structured problems that you come across in classes, your research just might not work. You could spend weeks working on some part of your project to have it yield no benefit whatsoever.\n                    This is certainly something I will need to get used to because such\n                    things happen in industry all of the time, but it certainly is frustrating the first time you come across it. Also, the lack of\n                    specifications can be rather tumultuous. If you are working on a\n                    project for an end user, you can narrow in your specifications by\n                    prototyping and then interviewing the end user. In research, the end user\n                    is.........well, nobody. We have a sponsor but the overall end goal is to expand human knowledge in a certain domain. But how that is done\n                    is open-ended. The specifications are chosen by you and are altered\n                    based on what works and what doesn't. The ambiguity can be quite\n                    burdensome in terms of scheduling how long any given part of the\n                    project will take. If you can't tell, I think doing research has been an\n                    invaluable experience, but I don't think it's for me.\n                    </li>\n                    <li>\n                    Auditing is super important, especially as the project gets more complicated. Auditing is kind of the same as test cases in coding,\n                    but they are distinct because they can't really be automated or\n                    even anticipated. In coding, the testing pipeline is \"this is what\n                    my app needs to do, I can write some tests ahead of time to validate\n                    those features and then develop until the product can pass all of the\n                    tests.\" Such a pipeline does not translate to large research projects. You can, and should, write test cases for your code, but research, and\n                    data science in general, needs something else. Many times, you will perform some sort of analysis and get results, and the auditing is\n                    going in and making sure that your analytical procedure is valid\n                    and that your results are reasonable. This can be a tedious process\n                    because it should really be done after any bit of analysis, but can't\n                    really be automated. You just have to do it.\n                    </li>\n                    </ul>\n\n                    <p>That's it for now. I will make another post later on in the semester wrapping up\n                    the project, as well as linking our published paper.<p>\n            ",
        "date": "2/10/2018",
        "location": "blog/2018/capstone1.html",
        "name": "Capstone Semester One Wrap Up (Part 2)"
    },
    {
        "content": "\n\n                    <p>This post is going to be a little bit different, and I hope this is a series that I can do more with. When most people think of data science, they think cutting edge deep learning and AI methodologies, and because of that it can seem a bit out of reach. People often forget that you can do some really fun stuff with data science using really simple, intuitive techniques. I want to do some posts on doing cool, data science-y things using simple concepts. This post is going to focus on generating text using a Markov Chain. This is inspired by the \"Random Kanye\" project, where people create models to generate random Kanye West tweets.</p>\n\n                    <p>I'm only going to briefly go over the statistics behind Markov Chains here, but for a really good background on this, listen to the \"Random Kanye\" episode of the <i>Linear Digressions</i> podcast (my current obsession). Basically, a Markov process is any process where the next state of the process is only dependent on the current state, not any of the past states. So, in the context of text generation, if I began the sentence \"I am from the United _____\", you could probably guess that the next word in that sentence is \"States\" and you could probably do it based only on the word \"United.\" Certainly, the other words help, but if I just said \"United _____\", you would probably still guess with reasonable certainty that the next word is \"States.\" This particular example displays the Markov property, since the next word depended only on the previous word, not any of the prior words. Now, a reasonable thing to point out would be that this doesn't always work. If I gave you the subset of a sentence, \"the _____\", you would be hardpressed to come up with the next word, since without context, almost any word can come after the word \"the.\"</p>\n\n                    <p>We can reconcile this by getting a large enough set of text and coming up with a probabilistic distribution for what words follow what other words. Essentially what this means is, scan over a large body of text looking at each pair of consecutive words, and keep track of how often words follow other words. So using the Random Kanye example from before, we might go through all his tweets and find out that the most common word that follows the word \"the\" is \"best\" (i.e, I am the best), while the next most common word is \"greatest\" (i.e, I am the greatest rapper). We would have a sense of how often each of these words follows the word \"the\". We would also have a sense of how often every other word in the vocabulary followed the world \"the\". Once we know how often every word in the vocabulary follows the world \"the\", we can easily turn this into a probability distribution by dividing by the total number of times we saw the word \"the\". So if we want to make a new Kanye tweet that starts with the word \"the\", we would just pick the next word in the tweet randomly accordingly to this probability distribution that we just defined. We would have a similar distribution for every word in the vocabulary, allowing us to probabilistically generate tweets from Kanye.</p>\n\n                    <p>Hopefully, this give you some sense of how we can use a Markov chain to generate new text from some corpus of training text. It turns out this works fairly well, and if you trained such a model on a large enough set of Kanye tweets, you would get decent results. It is important to note that since we are using Markov chains, these sentences have no memory. So while it often does an ok job, there is also a good chance that you just get incoherent rambling. Personally, I think these are the most fun because they are often hilarious. Now, the real fun in doing something like this is choosing your data source. In \"Random Kanye,\" the data source was Kanye tweets, but there is nothing limiting us to doing one sensible source. We can arbitrarily combine different data sources, and our model will treat them as one source, giving some pretty hilarious results.  For this post, we are going to combine the tweets of Donald Trump with the Bible. I'm not trying to be political in any way here, I just genuinely thought this combination would absurd. I am going to step through some of the code here, but by no means all of it. All of the code and data for this project is available <a href=\"https://github.com/bgreenawald/Markov-Text-Generation\" >here</a>.</p>\n\n                    <p>The first step is to get the data. I found a collection of about 30,000 <a href=\"https://github.com/mkearney/trumptweets\">Trump tweets</a> starting as long ago as 2009, and also found a text version of the <a href=\"http://www.gutenberg.org/ebooks/10>\">Guttenburg Bible</a>. I will largely omit the preprocessing details, but the pipeline was basically: remove any non-English characters, make all text lowercase, and replace endline characters will a special token. After this, I had a collection of preprocessed lines where each line was either a verse from Bible or a tweet from Donald Trump. I went ahead and pickled this list so I wouldn't have to keep redoing the preprocessing steps. For those unfamiliar, pickling is just Python's way of saving Python object for later use.</p>\n\n                    <p>Now that we have the data, the real fun can begin. First, we have to create the vocabulary. This just involves reading in every sentences, splitting on spaces, and saving all of the words to a list.</p>\n<pre class=\"prettyprint\"><code class=\"language-python\">\n# Create the vocabulary\nvocab = []\nfor line in text_full.split(\"\\n\"):\n    for word in line.split(\" \"):\n        vocab.append(word)\n        </code></pre>\n                    <p>This showed that the corpus had 1366285 total words, but after casting our list to a set (to remove duplicates), there were only 43714 unique words. Then, I create a mapping that maps every word to a number, and another that maps numbers to words.</p>\n                    <pre class=\"prettyprint\"><code class=\"language-python\">\n# Create a dict mapping vocab to index\nvocab_to_id = {}\nid_to_vocab = {}\nfor index, word in enumerate(vocab):\n    vocab_to_id[word] = index\n    id_to_vocab[index] = word\n                    </code></pre>\n                    <p>Finally, we can actually create our term frequency matrix. If <i>n</i> is the size of our vocabulary, this matrix is <i>n x n</i>, where each entry if how often the colum  word followed the row word. So if you found the number representing the word \"the\" (this number comes from the dictionary code above), and the number for the world \"president\" (call these number <i>x</i>, <i>y</i> respectivly), then the <i>x,y</i> entry of the term frequency matrix is how often the word \"president\" came after the word \"the.\" The code to make this matrix is below.</p>\n                    <pre class=\"prettyprint\"><code class=\"language-python\">\n# Fill up the tf matrix\nfor line in text_full.split(\"\\n\"):\n    words = line.split(\" \")\n    for i in range(len(words) - 1):\n        id1 = vocab_to_id[words[i]]\n        id2 = vocab_to_id[words[i + 1]]\n        tf[id1, id2] += 1\n                    </code></pre>\n                    <p>One useful implementation detail here is that your life will be a whole lot easier if at this point you convert the term frequency matrix to a sparse marix. Remember, this term frequency matrix is huge, but most of the entries are 0 since there are only so many valid pairing in the English language. Instead of making Python store this entire, mostly empty matrix, we can use a sparse matrix to save a ton of space. I personally use the scipy <i>coo</i> implementation, shown below. </p>\n                    <pre class=\"prettyprint\"><code class=\"language-python\">\nmat = sp.coo_matrix(tf)\n                    </code></pre>\n                    <p>Again, this is not required, but my relatively powerful laptop could not handle the code unless I did this, so it is effectively mandatory. </p>\n\n                    <p>A fun little aside, before we actually make new text, is to see what are the most common word pairings. I won't go through the code for this, but after finding the top 10 biggest elements matrix, I found that the 10 most common consecutive pairings of words are\n                    <ol style=\"padding: 0em 5em\">\n                        <li>of the</li>\n                        <li>the lord</li>\n                        <li>and the</li>\n                        <li>in the</li>\n                        <li>to the</li>\n                        <li>and he</li>\n                        <li>i will</li>\n                        <li>for the</li>\n                        <li>shall be</li>\n                        <li>all the</li>\n                    </ol>\n\n                    <p>As expected, these are pretty generic pairings of words. \"The lord\" is interesting because it is definitely Bible specific, but it is more common then even a pairing like \"and the\" that would appear in both the Bible and Trump tweets.</p>\n\n                    <p>Now, we are almost ready to actually make new text. The final step is to convert the term frequency matrix to a probability distribution (called a Markov or probability transition matrix). This is simply done by dividing every element in a row by the sum of the row.</p>\n                    <pre class=\"prettyprint\"><code class=\"language-python\">\n# Normalize the matrix\nmat_norm = normalize(mat, norm='l1', axis=1)\n                    </code></pre>\n\n                    <p>So now, we generate new text. To do this, we choose a start word, and using our probability transition matrix, keep adding words to the sentence until we hit an endline.</p>\n\n                    <pre class=\"prettyprint\"><code class=\"language-python\">\nsentence = start_word\nwhile start_word != \"endline\":\n    row_ind = vocab_to_id[start_word]\n    prob_dist = np.array(mat_norm.getrow(row_ind).todense())[0]\n    next_ind = np.random.choice(range(len(vocab)), p=prob_dist)\n    start_word = id_to_vocab[next_ind]\n    sentence += \" \" + start_word\n                    </code></pre>\n                    </p>\n\n        <p>Below are a couple of examples that I thought were fun. Using the code on Github, you can easily re-run this with whatever start word you desire.</p>\n\n                    <strong>Start word: 'the'</strong>\n                    <ul style=\"padding: 0em 5em\">\n                    <li>the nations shall many women working on the people when israel which are known loser who hath sent benaiah the white as i hope of macedonia and invent to the appearance they not nor taking over theaters this is mccarthyism</li>\n                    <li>the original costume was susan berry</li>\n                    <li>the midst of seventy years agothis is mad sometimes referred to be long massive tax just named eutychus being stubborn can call it</li>\n                    <li>the bar them that thou with his works nay but you have sinned against the lord</li>\n                    </ul>\n\n                    <strong>Start word: 'who'</strong>\n                    <ul style=\"padding: 0em 5em\">\n                    <li>who escaped alone and treasuries shelemiah shemariah and moab and this country is the one came up he defrauded us is through phenice and thou shalt not hearken unto aaron the obamacare</li>\n                    <li>who was a son of them thou shalt thou art not the word again despite obamas terrible</li>\n                    </ul>\n\n                    <strong>Start word: 'hillary'</strong>\n                    <ul style=\"padding: 0em 5em\">\n                    <li>hillary lying vanities but the goodness thou shalt hearken ye that were in all this day in twelve hours left to go way but of the lord of israel and her head</li>\n                    </ul>\n\n                    <p>So we can see a couple of interesting observations. The first is that because each word choice is random, we can use the same start word multiple times and get very different sentences. The second is that because this process has no memory, the sentence can start off very biblical and midway just switch to being quite Trumpian. This is the primary weakness for using Markov chains for text generation, but it's also why they are so much fun.</p>\n\n                    <p>This wraps up this little diversion. Hopefully, this demonstrates how using very simple statistical techniques can do something extremely complex like text generation (although arguably it doesn't very well, but still). I encourage you to use the code on Github to try new start words to make sentences. The pipeline I made it also fairly generic, so it can accommodate any text you throw at it. The next combinations I intend to try Trump tweets and the Quron and Trump tweets and the Torah to see how swapping the religious text alters the results. Hope you enjoyed!<p>\n\n\n            ",
        "date": "3/18/2018",
        "location": "blog/2018/markov-text-gen.html",
        "name": "Fun with Simple Data Science: Markov Text Generation"
    },
    {
        "content": "\n\n                    <p>In the previous post, we explored how to use Markov chains to generate new text, and did so through the lens of combining to different corpora of text (Trump tweets and the Bible). In this post, which is going to be a short follow-up, we are going to address one fundamental issue with the original post: that there was no sense of memory in the model. </p>\n\n                    <p>By definition, Markov chains cannot have memory. As a brief review, the definition of a Markov process is one in which the next state of the system relies only on the current state, not on any of the past states. This presents a bit of an issue with text generation since human text is definitely not Markovian. In order to predict what a person is going to say next, you need at least a couple of words of context, usually closer to a full sentence. This issue was in full display in the sentences the model was creating. A sentence would often start in the tone of the Bible and then suddenly switch over to being Trumpian, or vice-versa. Take for examples \"who was a son of them thou shalt thou art not the word again despite obamas terrible.\" This entire sentence reads like a grammatically atrocious Bible verse up until the last two words, where somehow Obama gets thrown into the mix. To get more cohesion into these sentences, we need to introduce some form of memory.</p>\n\n                    <p>As stated, we really can't do this within the framework of a traditional Markov model, we're going to have to improvise a little. I should note that most of what's going on in this post was just me messing around with extending the model and doesn't have the same solid mathematical backing as the previous post. Nevertheless, let's just jump into it.</p>\n\n                    <p>The task at hand is to take the code used from the previous post and extend it to use not one, but <i>n</i> words in order to predict the next word. Now the data science savvy amongst you are immediately jumping to \"sequence to sequence RNN models.\" This is certainly the preferred method and would likely get us much better results, but I wanted to try something a bit different. Since this is \"Fun with Simple Data Science,\" I wanted to try and improve the results using only the concept of probability distributions introduced last time. Using an example, let's look at the ideal way to go about this problem. Keeping with the Trump tweet example, let's say we had the starter text \"I am the _____\" and want to predict the next word. To answer this, I would need to answer three questions: what words usually come 3 words after the word \"I\", what words usually come 2 words after the word \"am\", and what words usually come 1 word after the word \"the.\" I would combine the answers to these three subquestions and use that to generate my prediction. Here in lies a problem. The code we have can only look ahead one word. So we simplify the problem a little bit and just say, what words usually come after \"I\", \"am\", and \"the\" separately, and combine this information to generate the final prediction.</p>\n\n                    <p>With this methodology, we can now take any number of words as an initial input and just get the probability distribution for each word. Now the question becomes how do we combine <i>n</i> probability distributions into a single probability distribution. The answer is that we're going to do a weighted sum of the probability distributions. Formally, the final probability distribution <i>w</i> that we will use to generate our predictions is $$w = w_0 * word_0 + w_1 * word_1 + .... + w_n * word_n$$ The question of what weights to use is an interesting one and one that I don't necessarily have a good answer for. I chose to just test out a few different ones and see what gave the best results. </p>\n\n                    <p>I tested three different weighting schemes. The first was uniform weighting, where each word was given an equal vote when deciding the next word.</p>\n\n                    <pre class=\"prettyprint\"><code class=\"language-python\">\nweights = np.zeros(len(start_words)) + 1\nweights = weights / len(start_words)\n                    </code></pre>\n\n                    <p>The next was an exponential decay, where the most recent word was given the highest weight, and as you went further back, words got exponentially less of a vote.</p>\n\n                    <pre class=\"prettyprint\"><code class=\"language-python\">\nweights = [0] * len(start_words)\nfor i in range(len(start_words)):\n    weights[i] = math.pow(gamma, i)\n\nweights.reverse()\nweights = np.array(weights)/sum(weights)\n                    </code></pre>\n\n                    <p>Finally, I did a random weighting where weights were just chosen randomly.</p>\n\n                    <pre class=\"prettyprint\"><code class=\"language-python\">\nweights = np.array(random.sample(range(0,100), len(start_words)))\nweights = weights / sum(weights)\n                    </code></pre>\n\n                    <p>With the weights well defined, we can now start generating text. The code, shown below, is similar to how we generated text last time. We take in <i>n</i> words, and for each word get the probability distribution for that word and multiply it by the corresponding weight. Doing this for every word and summing the results gives us the probability distribution for the next word, which we predict and append to our result sentence. Now the word we just generated is used in the next prediction, along with the previous <i>n - 1</i> words. Continue this process until we hit an endline, and output that as the result.</p> \n\n                    <pre class=\"prettyprint\"><code class=\"language-python\">\n# Number of steps to look back\nlookback = len(start_words)\n\nwhile current_word != \"endline\":\n    # Get the last \"n\" words, where \"n\" is the lookback amount\n    lookback_words = start_words[-(lookback):]\n\n    # Start with the furthest back word and use it's distribution as the start\n    row_ind = vocab_to_id[start_words[-len(start_words)]]\n    prob_dist = weights[0] * np.array(mat_norm.getrow(row_ind).todense())[0]\n\n    # For all the rest of the words, add the weighted probability distribution to the result\n    for i, word in enumerate(lookback_words[1:]):\n        row_ind = vocab_to_id[word]\n        prob_dist += weights[i + 1] * np.array(mat_norm.getrow(row_ind).todense())[0]\n\n    # Make sure we don't have repeats\n    current_word = start_words[-1]\n    while current_word in lookback_words:\n        next_ind = np.random.choice(range(len(vocab)), p=prob_dist)\n        current_word = id_to_vocab[next_ind]\n\n    # Append the predicted word the results\n    start_words.append(current_word)\n                    </code></pre>\n\n                    <p>Now for the fun part. The results for a couple of different initial phrases are shown below. The results also compare each of the three weighting schemes.</p>\n\n                    <table class=\"alt\">\n                        <tr>\n                            <th>Starting Words</th>\n                            <th class=\"tg-l711\">Weights</th>\n                            <th class=\"tg-us36\">Generated Text</th>\n                        </tr>\n                        <tr>\n                            <td rowspan=\"6\">donald trump is</td>\n                            <td rowspan=\"2\">decaying</td>\n                            <td >donald trump is thinking an big guy leads to finally someone realize you for years the holy one that o trump house thou shalt j trump jeb used the wilderness a trump you believe</td>\n                        </tr>\n                        <tr>\n                            <td>donald trump is u the re tax their credit trump for</td>\n                        </tr>\n                        <tr>\n                            <td rowspan=\"2\">uniform</td>\n                            <td>donald trump is at the trump president j jimmy trumps son</td>\n                        </tr>\n                        <tr>\n                            <td>donald trump is we are ye might j trump when ill be trump only run the trump house youll j trump you your words money trump no shepherd he of the your arguing enemies shall be take great thx</td>\n                        </tr>\n                        <tr>\n                            <td rowspan=\"2\">random</td>\n                            <td>donald trump is have you i run donald trump international unlike anything worth trump</td>\n                        </tr>\n                        <tr>\n                            <td>donald trump is now hear the children pot of trump for</td>\n                        </tr>\n                        <tr>\n                            <td rowspan=\"6\">though shalt not</td>\n                            <td rowspan=\"2\">decaying</td>\n                            <td >though shalt not bless the king hand of illegals to bring ye another will draw</td>\n                        </tr>\n                        <tr>\n                            <td>though shalt not sincerely increase supposing the world they say again thou being wilt twelve men they cause some i of am the best interviews and delaying his the bramble and abigail the nabals wife looked bare all my lips soul</td>\n                        </tr>\n                        <tr>\n                            <td rowspan=\"2\">uniform</td>\n                            <td>though shalt not i in will he i is want to big we i too he turned a their way kings counsel</td>\n                        </tr>\n                        <tr>\n                            <td>though shalt not save thou shalt gleaned he they that thou me art and i will cast they sank i the have jesus remained u debate would have love they the every tea party i had prayed brought my he i that despiseth atlantic thou make livest these things ten list the on foxandfriends this great i feel say the i will save bless alive thee he the rested themselves what in greensboro your north it country charles he missouri hath trump you they a lacked mother opportunity in any case i disgraceful always we on actually the prophets and im love from</td>\n                        </tr>\n                        <tr>\n                            <td rowspan=\"2\">random</td>\n                            <td>though shalt not go use twitter trump</td>\n                        </tr>\n                        <tr>\n                            <td>though shalt not he then let us the freeing super up tuesday morning on television a it is not enough repeal signatures fools big league crowd thou shalt love roast with and an i emotional apprenticenbc still her daughters times fools in feedeth thy among thorns all thy the matters who dennis opposes nd choice worst ye thing say who unto them absalom</td>\n                        </tr>\n                        <tr>\n                            <td rowspan=\"6\">i am the</td>\n                            <td rowspan=\"2\">decaying</td>\n                            <td >i am the heavenly have forsaken covenanted with no a samaritan as that trust forsake ye begin to the ships asher and they went in out time gave an iraq will it appoint is you trump wins will written heal like neither can could only in truth not their healthcare would plan will be totally proved did are change bring back will i need your to hand pretending of not spare them look and thou he wonder of where to hear used the win will went and he testifieth thou disquieted shalt win</td>\n                        </tr>\n                        <tr>\n                            <td>i am the have young havent did was being offered merciful and man feareth have god and with thee o hosted by andrewejohnson</td>\n                        </tr>\n                        <tr>\n                            <td rowspan=\"2\">uniform</td>\n                            <td>i am the ephraimites best which was great</td>\n                        </tr>\n                        <tr>\n                            <td>i am the never have smote i them could not try cannot to get trump it am persuaded the law kindred for yourselves therefore said thus love congratulate have him a full of would a prenup</td>\n                        </tr>\n                        <tr>\n                            <td rowspan=\"2\">random</td>\n                            <td>i am the pray will say rebuke am will might say think always made for sat follow gave wonder will love have know just shall was see have just scatter will say looked gave have i said am testified therefore let have unto am may understanding hope have will hate say said daughter shall did have wish love cant let that say go may take am will should dont wrote rejoice will am sent thank will delivered came run should say will am tell his pray saw am will pray know cannot am have had love appointed was will agree find pray go loved completely am agree said had might truly see send will am watched have know am wonder cut will want have shall again would tell will vote need play will darius totally speak may beheld them hope will am</td>\n                        </tr>\n                        <tr>\n                            <td>i am the three will vote was able to shur and never have consulted with will also have to comprehend</td>\n                        </tr>\n                    </table>\n\n                    <p>We wrap up with a couple of observations. First off,  it seems that in general, this method produces longer phrases than the original model. My guess is that this is due to a much more noise in the final probability distributions, making it less likely to hit the \"endline\" character at any given time. Secondly, it does not appear that increasing the lookback really did anything to rectify the problem we were trying to solve, which was cohesion in our predictions. It still seems to be the case that the predictions are tonally and thematically all over the place. I attribute this to an overall small lookback and the fact that the algorithm we use doesn't really tackle the problem in the way that we would like. Nevertheless, any model that dictates \"though shalt not go use twitter trump\" can't be seen as too huge a failure.</p>\n                    \n                    <p>The code for this project can be found <a href=\"https://github.com/bgreenawald/Markov-Text-Generation\" >here</a>.</p>\n\n                    ",
        "date": "6/15/2018",
        "location": "blog/2018/markov-text-gen2.html",
        "name": "Fun with Simple Data Science: Markov Text Generation (Part 2)"
    }
]